[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "importPath": "pandas",
        "description": "pandas",
        "isExtraImport": true,
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "importPath": "pandas",
        "description": "pandas",
        "isExtraImport": true,
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "Fitter",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "get_common_distributions",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "Fitter",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "get_common_distributions",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "get_distributions",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "HistFit",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "Fitter",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "get_common_distributions",
        "importPath": "fitter",
        "description": "fitter",
        "isExtraImport": true,
        "detail": "fitter",
        "documentation": {}
    },
    {
        "label": "variance_inflation_factor",
        "importPath": "statsmodels.stats.outliers_influence",
        "description": "statsmodels.stats.outliers_influence",
        "isExtraImport": true,
        "detail": "statsmodels.stats.outliers_influence",
        "documentation": {}
    },
    {
        "label": "variance_inflation_factor",
        "importPath": "statsmodels.stats.outliers_influence",
        "description": "statsmodels.stats.outliers_influence",
        "isExtraImport": true,
        "detail": "statsmodels.stats.outliers_influence",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "arange",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "asarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "hstack",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "vstack",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "percentile",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "rand",
        "importPath": "numpy.random",
        "description": "numpy.random",
        "isExtraImport": true,
        "detail": "numpy.random",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "importPath": "FS.functionHO",
        "description": "FS.functionHO",
        "isExtraImport": true,
        "detail": "FS.functionHO",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "KNeighborsClassifier",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "KNeighborsRegressor",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "KNeighborsRegressor",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "KNeighborsRegressor",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "LocalOutlierFactor",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "KNeighborsRegressor",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "statsmodels.api",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "statsmodels.api",
        "description": "statsmodels.api",
        "detail": "statsmodels.api",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "stepwiseSelection",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "stepwiseSelection",
        "description": "stepwiseSelection",
        "detail": "stepwiseSelection",
        "documentation": {}
    },
    {
        "label": "dlt_create_dir",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "dlt_create_dir",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "set_season_time",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "delete_any_duplicates",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "split_data_by_GW",
        "importPath": "utilities",
        "description": "utilities",
        "isExtraImport": true,
        "detail": "utilities",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "Lasso",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "ElasticNet",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "ElasticNet",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "IsolationForest",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "sklearn.metrics",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lr_preprocess",
        "description": "lr_preprocess",
        "isExtraImport": true,
        "detail": "lr_preprocess",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lr_preprocess",
        "description": "lr_preprocess",
        "isExtraImport": true,
        "detail": "lr_preprocess",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "lr_preprocess",
        "description": "lr_preprocess",
        "isExtraImport": true,
        "detail": "lr_preprocess",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "RandomizedSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "RepeatedKFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "KFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "RepeatedKFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "keras",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "keras",
        "description": "keras",
        "detail": "keras",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "keras.models",
        "description": "keras.models",
        "isExtraImport": true,
        "detail": "keras.models",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "keras.models",
        "description": "keras.models",
        "isExtraImport": true,
        "detail": "keras.models",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "keras.models",
        "description": "keras.models",
        "isExtraImport": true,
        "detail": "keras.models",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "keras.layers",
        "description": "keras.layers",
        "isExtraImport": true,
        "detail": "keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "keras.layers",
        "description": "keras.layers",
        "isExtraImport": true,
        "detail": "keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "keras.layers",
        "description": "keras.layers",
        "isExtraImport": true,
        "detail": "keras.layers",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "less",
        "importPath": "keras.backend.tensorflow_backend",
        "description": "keras.backend.tensorflow_backend",
        "isExtraImport": true,
        "detail": "keras.backend.tensorflow_backend",
        "documentation": {}
    },
    {
        "label": "set_session",
        "importPath": "keras.backend.tensorflow_backend",
        "description": "keras.backend.tensorflow_backend",
        "isExtraImport": true,
        "detail": "keras.backend.tensorflow_backend",
        "documentation": {}
    },
    {
        "label": "clear_session",
        "importPath": "keras.backend.tensorflow_backend",
        "description": "keras.backend.tensorflow_backend",
        "isExtraImport": true,
        "detail": "keras.backend.tensorflow_backend",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "keras.backend.tensorflow_backend",
        "description": "keras.backend.tensorflow_backend",
        "isExtraImport": true,
        "detail": "keras.backend.tensorflow_backend",
        "documentation": {}
    },
    {
        "label": "PlotLossesKeras",
        "importPath": "livelossplot",
        "description": "livelossplot",
        "isExtraImport": true,
        "detail": "livelossplot",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "numba",
        "description": "numba",
        "isExtraImport": true,
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "scale",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "OneClassSVM",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "generate_data",
        "importPath": "lr_model",
        "description": "lr_model",
        "isExtraImport": true,
        "detail": "lr_model",
        "documentation": {}
    },
    {
        "label": "register_matplotlib_converters",
        "importPath": "pandas.plotting",
        "description": "pandas.plotting",
        "isExtraImport": true,
        "detail": "pandas.plotting",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "Understat",
        "importPath": "understat",
        "description": "understat",
        "isExtraImport": true,
        "detail": "understat",
        "documentation": {}
    },
    {
        "label": "nest_asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nest_asyncio",
        "description": "nest_asyncio",
        "detail": "nest_asyncio",
        "documentation": {}
    },
    {
        "label": "progressbar",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "progressbar",
        "description": "progressbar",
        "detail": "progressbar",
        "documentation": {}
    },
    {
        "label": "difflib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "difflib",
        "description": "difflib",
        "detail": "difflib",
        "documentation": {}
    },
    {
        "label": "get_distributions",
        "importPath": "fitter.fitter",
        "description": "fitter.fitter",
        "isExtraImport": true,
        "detail": "fitter.fitter",
        "documentation": {}
    },
    {
        "label": "KerasRegressor",
        "importPath": "keras.wrappers.scikit_learn",
        "description": "keras.wrappers.scikit_learn",
        "isExtraImport": true,
        "detail": "keras.wrappers.scikit_learn",
        "documentation": {}
    },
    {
        "label": "KerasRegressor",
        "importPath": "keras.wrappers.scikit_learn",
        "description": "keras.wrappers.scikit_learn",
        "isExtraImport": true,
        "detail": "keras.wrappers.scikit_learn",
        "documentation": {}
    },
    {
        "label": "unique",
        "importPath": "numpy.lib.arraysetops",
        "description": "numpy.lib.arraysetops",
        "isExtraImport": true,
        "detail": "numpy.lib.arraysetops",
        "documentation": {}
    },
    {
        "label": "DecisionTreeRegressor",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeRegressor",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeRegressor",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "SelectKBest",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "f_regression",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectKBest",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectPercentile",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "mutual_info_regression",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectFromModel",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectKBest",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectKBest",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "SelectPercentile",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "VarianceThreshold",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "chi2",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "mutual_info_classif",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "RFECV",
        "importPath": "sklearn.feature_selection",
        "description": "sklearn.feature_selection",
        "isExtraImport": true,
        "detail": "sklearn.feature_selection",
        "documentation": {}
    },
    {
        "label": "_diff_dispatcher",
        "importPath": "numpy.lib.function_base",
        "description": "numpy.lib.function_base",
        "isExtraImport": true,
        "detail": "numpy.lib.function_base",
        "documentation": {}
    },
    {
        "label": "BorutaPy",
        "importPath": "boruta",
        "description": "boruta",
        "isExtraImport": true,
        "detail": "boruta",
        "documentation": {}
    },
    {
        "label": "BorutaPy",
        "importPath": "boruta",
        "description": "boruta",
        "isExtraImport": true,
        "detail": "boruta",
        "documentation": {}
    },
    {
        "label": "mrmr_regression",
        "importPath": "mrmr",
        "description": "mrmr",
        "isExtraImport": true,
        "detail": "mrmr",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "model_selection",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pandas.core.frame",
        "description": "pandas.core.frame",
        "isExtraImport": true,
        "detail": "pandas.core.frame",
        "documentation": {}
    },
    {
        "label": "clear_session",
        "importPath": "keras.backend",
        "description": "keras.backend",
        "isExtraImport": true,
        "detail": "keras.backend",
        "documentation": {}
    },
    {
        "label": "patch_sklearn",
        "importPath": "sklearnex",
        "description": "sklearnex",
        "isExtraImport": true,
        "detail": "sklearnex",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "make_regression",
        "importPath": "sklearn.datasets",
        "description": "sklearn.datasets",
        "isExtraImport": true,
        "detail": "sklearn.datasets",
        "documentation": {}
    },
    {
        "label": "SuperLearner",
        "importPath": "mlens.ensemble",
        "description": "mlens.ensemble",
        "isExtraImport": true,
        "detail": "mlens.ensemble",
        "documentation": {}
    },
    {
        "label": "Subset",
        "importPath": "mlens.preprocessing",
        "description": "mlens.preprocessing",
        "isExtraImport": true,
        "detail": "mlens.preprocessing",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "EllipticEnvelope",
        "importPath": "sklearn.covariance",
        "description": "sklearn.covariance",
        "isExtraImport": true,
        "detail": "sklearn.covariance",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "missingno",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "missingno",
        "description": "missingno",
        "detail": "missingno",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "corrplot",
        "importPath": "heatmap",
        "description": "heatmap",
        "isExtraImport": true,
        "detail": "heatmap",
        "documentation": {}
    },
    {
        "label": "heatmap",
        "importPath": "heatmap",
        "description": "heatmap",
        "isExtraImport": true,
        "detail": "heatmap",
        "documentation": {}
    },
    {
        "label": "PLSRegression",
        "importPath": "sklearn.cross_decomposition",
        "description": "sklearn.cross_decomposition",
        "isExtraImport": true,
        "detail": "sklearn.cross_decomposition",
        "documentation": {}
    },
    {
        "label": "infoselect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "infoselect",
        "description": "infoselect",
        "detail": "infoselect",
        "documentation": {}
    },
    {
        "label": "jfs",
        "importPath": "FS.pso",
        "description": "FS.pso",
        "isExtraImport": true,
        "detail": "FS.pso",
        "documentation": {}
    },
    {
        "label": "skrebate",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "skrebate",
        "description": "skrebate",
        "detail": "skrebate",
        "documentation": {}
    },
    {
        "label": "ReliefF",
        "importPath": "skrebate",
        "description": "skrebate",
        "isExtraImport": true,
        "detail": "skrebate",
        "documentation": {}
    },
    {
        "label": "sklearn_relief",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn_relief",
        "description": "sklearn_relief",
        "detail": "sklearn_relief",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "matplotlib.font_manager",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.font_manager",
        "description": "matplotlib.font_manager",
        "detail": "matplotlib.font_manager",
        "documentation": {}
    },
    {
        "label": "ABOD",
        "importPath": "pyod.models.abod",
        "description": "pyod.models.abod",
        "isExtraImport": true,
        "detail": "pyod.models.abod",
        "documentation": {}
    },
    {
        "label": "CBLOF",
        "importPath": "pyod.models.cblof",
        "description": "pyod.models.cblof",
        "isExtraImport": true,
        "detail": "pyod.models.cblof",
        "documentation": {}
    },
    {
        "label": "FeatureBagging",
        "importPath": "pyod.models.feature_bagging",
        "description": "pyod.models.feature_bagging",
        "isExtraImport": true,
        "detail": "pyod.models.feature_bagging",
        "documentation": {}
    },
    {
        "label": "HBOS",
        "importPath": "pyod.models.hbos",
        "description": "pyod.models.hbos",
        "isExtraImport": true,
        "detail": "pyod.models.hbos",
        "documentation": {}
    },
    {
        "label": "IForest",
        "importPath": "pyod.models.iforest",
        "description": "pyod.models.iforest",
        "isExtraImport": true,
        "detail": "pyod.models.iforest",
        "documentation": {}
    },
    {
        "label": "KNN",
        "importPath": "pyod.models.knn",
        "description": "pyod.models.knn",
        "isExtraImport": true,
        "detail": "pyod.models.knn",
        "documentation": {}
    },
    {
        "label": "LOF",
        "importPath": "pyod.models.lof",
        "description": "pyod.models.lof",
        "isExtraImport": true,
        "detail": "pyod.models.lof",
        "documentation": {}
    },
    {
        "label": "LOCI",
        "importPath": "pyod.models.loci",
        "description": "pyod.models.loci",
        "isExtraImport": true,
        "detail": "pyod.models.loci",
        "documentation": {}
    },
    {
        "label": "MCD",
        "importPath": "pyod.models.mcd",
        "description": "pyod.models.mcd",
        "isExtraImport": true,
        "detail": "pyod.models.mcd",
        "documentation": {}
    },
    {
        "label": "OCSVM",
        "importPath": "pyod.models.ocsvm",
        "description": "pyod.models.ocsvm",
        "isExtraImport": true,
        "detail": "pyod.models.ocsvm",
        "documentation": {}
    },
    {
        "label": "PCA",
        "importPath": "pyod.models.pca",
        "description": "pyod.models.pca",
        "isExtraImport": true,
        "detail": "pyod.models.pca",
        "documentation": {}
    },
    {
        "label": "SOS",
        "importPath": "pyod.models.sos",
        "description": "pyod.models.sos",
        "isExtraImport": true,
        "detail": "pyod.models.sos",
        "documentation": {}
    },
    {
        "label": "LSCP",
        "importPath": "pyod.models.lscp",
        "description": "pyod.models.lscp",
        "isExtraImport": true,
        "detail": "pyod.models.lscp",
        "documentation": {}
    },
    {
        "label": "COF",
        "importPath": "pyod.models.cof",
        "description": "pyod.models.cof",
        "isExtraImport": true,
        "detail": "pyod.models.cof",
        "documentation": {}
    },
    {
        "label": "SOD",
        "importPath": "pyod.models.sod",
        "description": "pyod.models.sod",
        "isExtraImport": true,
        "detail": "pyod.models.sod",
        "documentation": {}
    },
    {
        "label": "thundersvm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "thundersvm",
        "description": "thundersvm",
        "detail": "thundersvm",
        "documentation": {}
    },
    {
        "label": "SVR",
        "importPath": "thundersvm",
        "description": "thundersvm",
        "isExtraImport": true,
        "detail": "thundersvm",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "mktime",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "elements",
        "kind": 5,
        "importPath": "data.2020-21.latex",
        "description": "data.2020-21.latex",
        "peekOfCode": "elements = pd.read_csv('./players_raw.csv')\nprint(elements[['first_name', 'second_name', 'transfers_in', 'red_cards']].head(5).to_latex(index=False))\n# %%\na\n# %%",
        "detail": "data.2020-21.latex",
        "documentation": {}
    },
    {
        "label": "plot_feat",
        "kind": 2,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "def plot_feat(df, feat):\n    sns.set_style('white')\n    sns.set_context(\"paper\", font_scale = 2)\n    sns.displot(data=df, x=feat, kind=\"hist\", bins = 100, aspect = 1.5)\n# %%\n# plot_feat(df, 'ict_index')\n# %%\npd.read_csv()\n# %%\n# %%",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "check_data_dist",
        "kind": 2,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "def check_data_dist(df, feat = None):\n    if feat is None:\n        for feat in df.select_dtypes(['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns:\n            dist_fit = Fitter(df[feat], timeout=60*10, distributions=get_common_distributions())\n            dist_fit.fit()\n            # dist_fit.summary() \n            key, value = list(dist_fit.get_best(method = 'sumsquare_error').items())[0] # Key is the identified distribution\n            print(f'{feat} has {key} distribution')\n    else:\n            dist_fit = Fitter(df[feat], timeout=60*10, distributions=get_common_distributions())",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "calculate_vif",
        "kind": 2,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "def calculate_vif(X):\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\ncalculate_vif(df.select_dtypes(['int16', 'int32', 'int64', 'float16', 'float32', 'float64']))\n# %%",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//cleaned_fpl.csv', index_col=0)\ndf_us = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//cleaned_understat.csv', index_col=0)\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# Plot histogram\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# sns.set_style('white')\n# sns.set_context(\"paper\", font_scale = 2)\n# sns.displot(data=df, x=\"total_points\", kind=\"hist\", bins = 100, aspect = 1.5)\n# %%",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "df_us",
        "kind": 5,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "df_us = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//cleaned_understat.csv', index_col=0)\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# Plot histogram\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# sns.set_style('white')\n# sns.set_context(\"paper\", font_scale = 2)\n# sns.displot(data=df, x=\"total_points\", kind=\"hist\", bins = 100, aspect = 1.5)\n# %%\ndef plot_feat(df, feat):",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "dist_fit",
        "kind": 5,
        "importPath": "misc.check_distrib",
        "description": "misc.check_distrib",
        "peekOfCode": "dist_fit = Fitter(df['total_points'], timeout=60*10, distributions=get_common_distributions())\ndist_fit.fit()\ndist_fit.summary()\n# %%\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef calculate_vif(X):\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]",
        "detail": "misc.check_distrib",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.ba",
        "description": "src.FS.ba",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.ba",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.ba",
        "description": "src.FS.ba",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.ba",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.ba",
        "description": "src.FS.ba",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0",
        "detail": "src.FS.ba",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.ba",
        "description": "src.FS.ba",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0\n    thres  = 0.5\n    fmax   = 2      # maximum frequency\n    fmin   = 0      # minimum frequency\n    alpha  = 0.9    # constant\n    gamma  = 0.9    # constant\n    A_max  = 2      # maximum loudness",
        "detail": "src.FS.ba",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.cs",
        "description": "src.FS.cs",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.cs",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.cs",
        "description": "src.FS.cs",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.cs",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.cs",
        "description": "src.FS.cs",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\n# Levy Flight\ndef levy_distribution(beta, dim):\n    # Sigma     \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)",
        "detail": "src.FS.cs",
        "documentation": {}
    },
    {
        "label": "levy_distribution",
        "kind": 2,
        "importPath": "src.FS.cs",
        "description": "src.FS.cs",
        "peekOfCode": "def levy_distribution(beta, dim):\n    # Sigma     \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n    sigma = (nume / deno) ** (1 / beta) \n    # Parameter u & v \n    u     = np.random.randn(dim) * sigma\n    v     = np.random.randn(dim)\n    # Step \n    step  = u / abs(v) ** (1 / beta)",
        "detail": "src.FS.cs",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.cs",
        "description": "src.FS.cs",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0\n    thres  = 0.5\n    Pa     = 0.25     # discovery rate\n    alpha  = 1        # constant\n    beta   = 1.5      # levy component\n    N          = opts['N']\n    max_iter   = opts['T']",
        "detail": "src.FS.cs",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.de",
        "description": "src.FS.de",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.de",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.de",
        "description": "src.FS.de",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.de",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.de",
        "description": "src.FS.de",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.de",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.de",
        "description": "src.FS.de",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    CR    = 0.9     # crossover rate\n    F     = 0.5     # factor\n    N        = opts['N']\n    max_iter = opts['T']\n    if 'CR' in opts:",
        "detail": "src.FS.de",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.fa",
        "description": "src.FS.fa",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.fa",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.fa",
        "description": "src.FS.fa",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.fa",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.fa",
        "description": "src.FS.fa",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0",
        "detail": "src.FS.fa",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.fa",
        "description": "src.FS.fa",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0\n    thres  = 0.5\n    alpha  = 1       # constant\n    beta0  = 1       # light amplitude\n    gamma  = 1       # absorbtion coefficient\n    theta  = 0.97    # control alpha\n    N          = opts['N']",
        "detail": "src.FS.fa",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.fpa",
        "description": "src.FS.fpa",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.fpa",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.fpa",
        "description": "src.FS.fpa",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.fpa",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.fpa",
        "description": "src.FS.fpa",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\n# Levy Flight\ndef levy_distribution(beta, dim):\n    # Sigma     \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)",
        "detail": "src.FS.fpa",
        "documentation": {}
    },
    {
        "label": "levy_distribution",
        "kind": 2,
        "importPath": "src.FS.fpa",
        "description": "src.FS.fpa",
        "peekOfCode": "def levy_distribution(beta, dim):\n    # Sigma     \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n    sigma = (nume / deno) ** (1 / beta) \n    # Parameter u & v \n    u     = np.random.randn(dim) * sigma\n    v     = np.random.randn(dim)\n    # Step \n    step  = u / abs(v) ** (1 / beta)",
        "detail": "src.FS.fpa",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.fpa",
        "description": "src.FS.fpa",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0\n    thres  = 0.5\n    beta   = 1.5    # levy component\n    P      = 0.8    # switch probability\n    N        = opts['N']\n    max_iter = opts['T']\n    if 'P' in opts:",
        "detail": "src.FS.fpa",
        "documentation": {}
    },
    {
        "label": "error_rate",
        "kind": 2,
        "importPath": "src.FS.functionHO",
        "description": "src.FS.functionHO",
        "peekOfCode": "def error_rate(xtrain, ytrain, x, opts):\n    # parameters\n    k     = opts['k']\n    fold  = opts['fold']\n    xt    = fold['xt']\n    yt    = fold['yt']\n    xv    = fold['xv']\n    yv    = fold['yv']\n    # Number of instances\n    num_train = np.size(xt, 0)",
        "detail": "src.FS.functionHO",
        "documentation": {}
    },
    {
        "label": "Fun",
        "kind": 2,
        "importPath": "src.FS.functionHO",
        "description": "src.FS.functionHO",
        "peekOfCode": "def Fun(xtrain, ytrain, x, opts):\n    # Parameters\n    alpha    = 0.99\n    beta     = 1 - alpha\n    # Original feature size\n    max_feat = len(x)\n    # Number of selected features\n    num_feat = np.sum(x == 1)\n    # Solve if no feature selected\n    if num_feat == 0:",
        "detail": "src.FS.functionHO",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.ga",
        "description": "src.FS.ga",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.ga",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.ga",
        "description": "src.FS.ga",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef roulette_wheel(prob):",
        "detail": "src.FS.ga",
        "documentation": {}
    },
    {
        "label": "roulette_wheel",
        "kind": 2,
        "importPath": "src.FS.ga",
        "description": "src.FS.ga",
        "peekOfCode": "def roulette_wheel(prob):\n    num = len(prob)\n    C   = np.cumsum(prob)\n    P   = rand()\n    for i in range(num):\n        if C[i] > P:\n            index = i;\n            break\n    return index\ndef jfs(xtrain, ytrain, opts):",
        "detail": "src.FS.ga",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.ga",
        "description": "src.FS.ga",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub       = 1\n    lb       = 0\n    thres    = 0.5    \n    CR       = 0.8     # crossover rate\n    MR       = 0.01    # mutation rate\n    N        = opts['N']\n    max_iter = opts['T']\n    if 'CR' in opts:",
        "detail": "src.FS.ga",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.gwo",
        "description": "src.FS.gwo",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.gwo",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.gwo",
        "description": "src.FS.gwo",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.gwo",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.gwo",
        "description": "src.FS.gwo",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.gwo",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.gwo",
        "description": "src.FS.gwo",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    N        = opts['N']\n    max_iter = opts['T']\n    # Dimension\n    dim = np.size(xtrain, 1)\n    if np.size(lb) == 1:",
        "detail": "src.FS.gwo",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.hho",
        "description": "src.FS.hho",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.hho",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.hho",
        "description": "src.FS.hho",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.hho",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.hho",
        "description": "src.FS.hho",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef levy_distribution(beta, dim):\n    # Sigma \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)",
        "detail": "src.FS.hho",
        "documentation": {}
    },
    {
        "label": "levy_distribution",
        "kind": 2,
        "importPath": "src.FS.hho",
        "description": "src.FS.hho",
        "peekOfCode": "def levy_distribution(beta, dim):\n    # Sigma \n    nume  = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n    deno  = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)\n    sigma = (nume / deno) ** (1 / beta)\n    # Parameter u & v \n    u     = np.random.randn(dim) * sigma\n    v     = np.random.randn(dim)\n    # Step \n    step  = u / abs(v) ** (1 / beta)",
        "detail": "src.FS.hho",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.hho",
        "description": "src.FS.hho",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    beta  = 1.5    # levy component\n    N        = opts['N']\n    max_iter = opts['T']\n    if 'beta' in opts:\n        beta = opts['beta']",
        "detail": "src.FS.hho",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.ja",
        "description": "src.FS.ja",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.ja",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.ja",
        "description": "src.FS.ja",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.ja",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.ja",
        "description": "src.FS.ja",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0",
        "detail": "src.FS.ja",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.ja",
        "description": "src.FS.ja",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub     = 1\n    lb     = 0\n    thres  = 0.5\n    N          = opts['N']\n    max_iter   = opts['T']\n    # Dimension\n    dim = np.size(xtrain, 1)\n    if np.size(lb) == 1:",
        "detail": "src.FS.ja",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.pso",
        "description": "src.FS.pso",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef init_velocity(lb, ub, N, dim):\n    V    = np.zeros([N, dim], dtype='float')\n    Vmax = np.zeros([1, dim], dtype='float')\n    Vmin = np.zeros([1, dim], dtype='float')",
        "detail": "src.FS.pso",
        "documentation": {}
    },
    {
        "label": "init_velocity",
        "kind": 2,
        "importPath": "src.FS.pso",
        "description": "src.FS.pso",
        "peekOfCode": "def init_velocity(lb, ub, N, dim):\n    V    = np.zeros([N, dim], dtype='float')\n    Vmax = np.zeros([1, dim], dtype='float')\n    Vmin = np.zeros([1, dim], dtype='float')\n    # Maximum & minimum velocity\n    for d in range(dim):\n        Vmax[0,d] = (ub[0,d] - lb[0,d]) / 2\n        Vmin[0,d] = -Vmax[0,d]\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.pso",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.pso",
        "description": "src.FS.pso",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.pso",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.pso",
        "description": "src.FS.pso",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.pso",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.pso",
        "description": "src.FS.pso",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    w     = 0.9    # inertia weight\n    c1    = 2      # acceleration factor\n    c2    = 2      # acceleration factor\n    N        = opts['N']\n    max_iter = opts['T']",
        "detail": "src.FS.pso",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.sca",
        "description": "src.FS.sca",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.sca",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.sca",
        "description": "src.FS.sca",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.sca",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.sca",
        "description": "src.FS.sca",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.sca",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.sca",
        "description": "src.FS.sca",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    alpha = 2       # constant\n    N         = opts['N']\n    max_iter  = opts['T']\n    if 'alpha' in opts:\n        alpha = opts['alpha'] ",
        "detail": "src.FS.sca",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.ssa",
        "description": "src.FS.ssa",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.ssa",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.ssa",
        "description": "src.FS.ssa",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.ssa",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.ssa",
        "description": "src.FS.ssa",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.ssa",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.ssa",
        "description": "src.FS.ssa",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    N        = opts['N']\n    max_iter = opts['T']\n    # Dimension\n    dim = np.size(xtrain, 1)\n    if np.size(lb) == 1:",
        "detail": "src.FS.ssa",
        "documentation": {}
    },
    {
        "label": "init_position",
        "kind": 2,
        "importPath": "src.FS.woa",
        "description": "src.FS.woa",
        "peekOfCode": "def init_position(lb, ub, N, dim):\n    X = np.zeros([N, dim], dtype='float')\n    for i in range(N):\n        for d in range(dim):\n            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * rand()        \n    return X\ndef binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):",
        "detail": "src.FS.woa",
        "documentation": {}
    },
    {
        "label": "binary_conversion",
        "kind": 2,
        "importPath": "src.FS.woa",
        "description": "src.FS.woa",
        "peekOfCode": "def binary_conversion(X, thres, N, dim):\n    Xbin = np.zeros([N, dim], dtype='int')\n    for i in range(N):\n        for d in range(dim):\n            if X[i,d] > thres:\n                Xbin[i,d] = 1\n            else:\n                Xbin[i,d] = 0\n    return Xbin\ndef boundary(x, lb, ub):",
        "detail": "src.FS.woa",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 2,
        "importPath": "src.FS.woa",
        "description": "src.FS.woa",
        "peekOfCode": "def boundary(x, lb, ub):\n    if x < lb:\n        x = lb\n    if x > ub:\n        x = ub\n    return x\ndef jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0",
        "detail": "src.FS.woa",
        "documentation": {}
    },
    {
        "label": "jfs",
        "kind": 2,
        "importPath": "src.FS.woa",
        "description": "src.FS.woa",
        "peekOfCode": "def jfs(xtrain, ytrain, opts):\n    # Parameters\n    ub    = 1\n    lb    = 0\n    thres = 0.5\n    b     = 1       # constant\n    N        = opts['N']\n    max_iter = opts['T']\n    if 'b' in opts:\n        b    = opts['b']",
        "detail": "src.FS.woa",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def read_data(season):\n    \"\"\"[This function reads the data]\n    Args:\n        season ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    fpl = pd.read_csv(training_path + 'shifted_fpl.csv')\n    understat = pd.read_csv(training_path + 'shifted_us.csv')",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "backward_elimination",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def backward_elimination(df, data_str, threshold_in = 0.05, verbose = True):\n    \"\"\"[This function recursively eliminates features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]\n        verbose (bool, optional): [description]. Defaults to True.\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    data_str = 'fpl'",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def write_log(iterations_log, data_str, elim_crit, direction): #TODO: incorporate me\n    log_dir = 'C://Users//jd-vz//Desktop//Code//src//models//lin_reg//misc//'\n    iterations_file = open(log_dir + f'{data_str}_{elim_crit}_{direction}_select_logs.txt',\"w+\") \n    iterations_file.write(iterations_log)\n    iterations_file.close()\ndef forward_elimination(df, data_str, threshold_in = 0.05, verbose=True):\n    \"\"\"[This function recursively adds features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "forward_elimination",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def forward_elimination(df, data_str, threshold_in = 0.05, verbose=True):\n    \"\"\"[This function recursively adds features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]\n        verbose (bool, optional): [description]. Defaults to True.\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    X = df.drop(columns= ['total_points', 'player_name', 'kickoff_time', 'GW'])",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "stepwise_select",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def stepwise_select(df, data_str, type, elim_crit):\n    \"\"\"[This model utilizes Apache's forward and backward selection based on three different criterion]\n    Args:\n        data_str ([type]): [description]\n        type ([type]): [description]\n        elim_crit ([type]): [description]  [\"adjr2\", \"aic\", \"bic\", \"r2\"]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    X = df.drop(columns= ['total_points', 'player_name', 'kickoff_time', 'GW'])",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "collect_features",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def collect_features(be_t, fe_t, be_r2, fe_r2, be_adjr2, fe_adjr2, fe_AIC, be_AIC ,fe_BIC, be_BIC):\n    lst = be_t +  fe_t +  be_r2 +  fe_r2 +  be_adjr2 +  fe_adjr2 +  fe_AIC +  be_AIC  + fe_BIC +  be_BIC\n    selected = pd.DataFrame(Counter(lst).most_common())\n    selected = selected[selected[0] != 'intercept']\n    selected.rename(columns = {0:'Feature', 1:'Occurences'}, inplace = True)\n    return selected\ndef one_hot_encode(fpl):\n    \"\"\"[This function one hot encodes the four categorical features into dummy variables]\n    Args:\n        fpl ([type]): [description]",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def one_hot_encode(fpl):\n    \"\"\"[This function one hot encodes the four categorical features into dummy variables]\n    Args:\n        fpl ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    fpl = pd.get_dummies(fpl, columns=['was_home', 'position', 'team_h', 'team_a'], \n                      prefix=['home', 'position', 'team_h', 'team_a'])\n    fpl.drop(columns=['home_False'], axis=1, inplace=True)",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "feature_selection",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def feature_selection(df, data_str, required_votes):\n    data = one_hot_encode(df)\n    log_dir = 'C://Users//jd-vz//Desktop//Code//src//models//lin_reg//misc//'\n    be_t = backward_elimination(data, data_str)\n    fe_t = forward_elimination(data, data_str) \n    be_r2 = stepwise_select(data, data_str, 'backward', 'r2') \n    fe_r2 = stepwise_select(data, data_str, 'forward', 'r2') \n    be_adjr2 = stepwise_select(data, data_str, 'backward', 'adjr2') \n    fe_adjr2 = stepwise_select(data, data_str, 'forward', 'adjr2') \n    be_AIC = stepwise_select(data, data_str, 'backward', 'AIC') ",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_feat_selec",
        "description": "src.models.lin_reg.lr_feat_selec",
        "peekOfCode": "def main(required_votes):\n    # TODO: Add averages and lag data to the match\n    fpl, understat = read_data(season = '2019-20') # Season stays constant for feature selection\n    feature_selection(fpl, 'fpl', required_votes)\n    feature_selection(understat, 'understat', required_votes)\n    # feature_selection(imp, 'imp', required_votes)\nif __name__ == '__main__':\n    main(required_votes = 3) # 3/6 of models need to vote for a feature)\n    print('Success')\n# %%",
        "detail": "src.models.lin_reg.lr_feat_selec",
        "documentation": {}
    },
    {
        "label": "regression_results",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def regression_results(y_true, y_pred, model_string):\n    # Regression metrics\n    print('# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>')\n    print(model_string)\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n    print('explained_variance: ', round(explained_variance,4))    ",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def plot_results(true_value, predicted_value, title):\n    plt.figure(figsize=(10,10))\n    plt.scatter(true_value, predicted_value, c='crimson')\n    p1 = max(max(predicted_value), max(true_value))\n    p2 = min(min(predicted_value), min(true_value))\n    plt.plot([p1, p2], [p1, p2], 'b-')\n    plt.xlabel('True Values', fontsize=15)\n    plt.ylabel('Predictions', fontsize=15)\n    plt.title(title)\n    plt.axis('equal')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_LR_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_LR_model(X_train, y_train, X_test, y_test, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = LinearRegression(normalize=True,n_jobs=-1)\n    unscaled_metrics(regressor, X_train, y_train, X_test, y_test, GW, 'LR')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_SVR_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_SVR_model(X_train, y_train, X_test, y_test, GW):\n    \"\"\"[This function tests a support vector regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was not scaled \n    unscaled_metrics(regressor, X_train, y_train, X_test, y_test, GW, 'SVR')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_RF_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_RF_model(X_train, y_train, X_test, y_test, GW):\n    \"\"\"[This function tests a random forest model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',\n                      min_samples_split=5, n_estimators=600)",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "unscaled_metrics",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def unscaled_metrics(regressor, X_train, y_train, X_test, y_test, GW, model_str):\n    regressor.fit(X_train, y_train)\n    y_pred_test = regressor.predict(X_test)\n    y_pred_test = np.round(y_pred_test) \n    y_pred_train = regressor.predict(X_train)\n    y_pred_train = np.round(y_pred_train) \n    regression_results(y_test, y_pred_test, model_str)\n    plot_results(y_test, y_pred_test, title = f'{model_str}: Testing, 2020 Gameweek {GW}')\n    regression_results(y_train, y_pred_train, model_str)\n    plot_results(y_train, y_pred_train, title = f'{model_str}: Training, 2019 + 2020 GW: {GW - 1}')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "scaled_metrics",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def scaled_metrics(regressor, X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW, model_str, epochs = None):\n    if model_str is 'Scaled Net':\n        regressor.fit(X_train, y_train, epochs=20)\n    else:\n        regressor.fit(X_train, y_train)\n    y_train_rescaled = np.round(std_scale_Y_train.inverse_transform(y_train))\n    y_test_rescaled = np.round(std_scale_Y_test.inverse_transform(y_test))\n    y_pred_test = np.round(std_scale_Y_test.inverse_transform(regressor.predict(X_test)))\n    y_pred_train = np.round(std_scale_Y_train.inverse_transform(regressor.predict(X_train)))\n    regression_results(y_test_rescaled, y_pred_test, model_str)",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_scaled_LR_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_scaled_LR_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = LinearRegression(normalize=False,n_jobs=-1)\n    scaled_metrics(regressor, X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW, 'Scaled LR')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_scaled_SVR_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_scaled_SVR_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was fitted to scaled data\n    scaled_metrics(regressor, X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW, 'Scaled SVR')",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_scaled_RF_model",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',\n                      min_samples_split=5, n_estimators=600)  ",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "plot_history",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Square Error')\n    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n    plt.legend()\n    plt.ylim([0,0.05])",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_scaled_net",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_scaled_net(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW):\n    \"\"\"[This is very much just a]\n    Args:\n        X_train ([type]): [description]\n        y_train ([type]): [description]\n        X_test ([type]): [description]\n        y_test ([type]): [description]\n        std_scale_Y_train ([type]): [description]\n        std_scale_Y_test ([type]): [description]\n        GW ([type]): [description]",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "split_data",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def split_data(df, df_test, GW):\n    df = df.append(df_test[df_test['GW'] < GW])\n    df_test = df_test[df_test['GW'] == GW] \n    X_train = df.drop(columns = ['player_name', 'GW', 'total_points']) #, 'creativity', 'ict_index', 'influence', 'threat'])\n    y_train = df['total_points']\n    X_test = df_test.drop(columns = ['player_name', 'GW', 'total_points']) #, 'creativity', 'ict_index', 'influence', 'threat'])\n    y_test = df_test['total_points']\n    return X_train, y_train, X_test, y_test\ndef generate_data(data_str, GW, scale, outlier_rem = False):\n    if scale is True:",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "generate_data",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def generate_data(data_str, GW, scale, outlier_rem = False):\n    if scale is True:\n        df, std_scale_X_train, std_scale_Y_train, scl_feat_train = preprocess_data(data_str, '2019-20', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        df_test, std_scale_X_test, std_scale_Y_test, scl_feat_test  = preprocess_data(data_str, '2020-21', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        X_train, y_train, X_test, y_test = split_data(df, df_test, GW)\n        return X_train, y_train, X_test, y_test, std_scale_X_train, std_scale_Y_train, scl_feat_train, std_scale_X_test, std_scale_Y_test, scl_feat_test\n    elif scale is False:\n        df = preprocess_data(data_str, '2019-20', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        df_test = preprocess_data(data_str, '2020-21', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        X_train, y_train, X_test, y_test = split_data(df, df_test, GW)",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_scaled_models",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_scaled_models(data_str, GW):\n    X_train, y_train, X_test, y_test, std_scale_X_train, \\\n        std_scale_Y_train, scl_feat_train, std_scale_X_test, \\\n            std_scale_Y_test, scl_feat_test = generate_data(data_str, GW = GW, scale = True, outlier_rem=False)\n    print(X_train.shape, X_test.shape)\n    test_scaled_LR_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW = GW)\n    # test_scaled_SVR_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW = GW)   # Takes ~5 min\n    # test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW = GW)  \n    # test_scaled_net(X_train, y_train, X_test, y_test, std_scale_Y_train, std_scale_Y_test, GW)\n    # #! SARIMAX, fireTS",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "test_normal_models",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "def test_normal_models(GW):\n    X_train, y_train, X_test, y_test = generate_data(data_str, GW =GW, scale = False)\n    test_LR_model(X_train, y_train, X_test, y_test, GW = GW)\n    # test_SVR_model(X_train, y_train, X_test, y_test, GW = GW) # Using scaled values\n    test_RF_model(X_train, y_train, X_test, y_test, GW = GW)\n# %%\n# test_normal_models(GW=1)\ntest_scaled_models(data_str='fpl', GW = 1)\n# %%\n# test_scaled_models(data_str='understat', GW = 1)",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "fpl",
        "kind": 5,
        "importPath": "src.models.lin_reg.lr_model",
        "description": "src.models.lin_reg.lr_model",
        "peekOfCode": "fpl = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//shifted_fpl.csv')\n# %%",
        "detail": "src.models.lin_reg.lr_model",
        "documentation": {}
    },
    {
        "label": "intersect",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_preprocess",
        "description": "src.models.lin_reg.lr_preprocess",
        "peekOfCode": "def intersect(a, b):\n    \"\"\"[This function finds the intersection between two player name columns]\n    Args:\n        a ([type]): [description]\n        b ([type]): [description]\n    Returns:\n        [type]: [The intersection]\n    \"\"\"    \n    # print(len(list(set(a) & set(b))), 'unique and matching names between FPL and Understat')\n    return list(set(a) & set(b))",
        "detail": "src.models.lin_reg.lr_preprocess",
        "documentation": {}
    },
    {
        "label": "scale_numeric",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_preprocess",
        "description": "src.models.lin_reg.lr_preprocess",
        "peekOfCode": "def scale_numeric(df):\n    std_scaler_X = StandardScaler()\n    std_scaler_Y = StandardScaler()\n    df_scaled = pd.DataFrame(std_scaler_X.fit_transform(df.drop(columns = ['total_points'], axis = 1).values), columns=df.drop(columns = ['total_points'], axis = 1).columns, index=df.index)\n    df_scaled['total_points'] = std_scaler_Y.fit_transform(df['total_points'].to_numpy().reshape(-1, 1))\n    return df_scaled, std_scaler_X, std_scaler_Y\ndef outliers_removal(df, type):\n    # NB: This function removes too many outlying points and should be reconsiderd\n    #! Assumes normal distribution\n    #* Package that investigates the data distribution... exp, beta, gamma... technomatex",
        "detail": "src.models.lin_reg.lr_preprocess",
        "documentation": {}
    },
    {
        "label": "outliers_removal",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_preprocess",
        "description": "src.models.lin_reg.lr_preprocess",
        "peekOfCode": "def outliers_removal(df, type):\n    # NB: This function removes too many outlying points and should be reconsiderd\n    #! Assumes normal distribution\n    #* Package that investigates the data distribution... exp, beta, gamma... technomatex\n    #* Exp --> upper qnt\n    numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.select_dtypes(include=numeric_types).columns:\n        if type == 'IQR':\n            Q1= df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)",
        "detail": "src.models.lin_reg.lr_preprocess",
        "documentation": {}
    },
    {
        "label": "valid_features",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_preprocess",
        "description": "src.models.lin_reg.lr_preprocess",
        "peekOfCode": "def valid_features(data_str, lin_reg_dir):\n    df_20 = one_hot_encode(pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//' + f'shifted_{data_str}.csv')).columns\n    df_21 = one_hot_encode(pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//' + f'shifted_{data_str}.csv')).columns\n    feats = pd.read_csv(lin_reg_dir + f'misc//selected_{data_str}_features.csv', index_col=0) # Linear Regression selected features\n    feat_pool = intersect(df_20, df_21)\n    valid_feats = intersect(feats.Feature, feat_pool)\n    valid_feats = valid_feats + ['total_points'] # Features within both datasets, and the target\n    return valid_feats\ndef preprocess_data(data_str, season, OHE = True, FEAT = True, OUTLIER = None, SCL = True):\n    #! Confirm the order of this program with Thorstens ",
        "detail": "src.models.lin_reg.lr_preprocess",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "src.models.lin_reg.lr_preprocess",
        "description": "src.models.lin_reg.lr_preprocess",
        "peekOfCode": "def preprocess_data(data_str, season, OHE = True, FEAT = True, OUTLIER = None, SCL = True):\n    #! Confirm the order of this program with Thorstens \n    #! Linear Regression: One hot encode categorical --> Apply Feature Selection Results --> Remove Outliers --> Scale all features\n    train_dir = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//' \n    lin_reg_dir = 'C://Users//jd-vz//Desktop//Code//src//models//lin_reg//'\n    df = pd.read_csv(train_dir + f'shifted_{data_str}.csv')\n    if OHE == True:\n        df = one_hot_encode(df)\n    if FEAT == True:\n       val_feat = valid_features('fpl', lin_reg_dir)",
        "detail": "src.models.lin_reg.lr_preprocess",
        "documentation": {}
    },
    {
        "label": "svc_param_selection",
        "kind": 2,
        "importPath": "src.models.svr.svr_model",
        "description": "src.models.svr.svr_model",
        "peekOfCode": "def svc_param_selection(X, y, nfolds):\n    # Poly + RBF\n    Cs = [0.1, 1, 10, 100]\n    gammas = [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n    epsilons = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n    param_grid = {'C': Cs, 'gamma' : gammas, 'epsilon' : epsilons}\n    grid_search = GridSearchCV(SVR(verbose=3), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    # grid_search.best_params_\n    return grid_search",
        "detail": "src.models.svr.svr_model",
        "documentation": {}
    },
    {
        "label": "grid_search",
        "kind": 5,
        "importPath": "src.models.svr.svr_model",
        "description": "src.models.svr.svr_model",
        "peekOfCode": "grid_search = svc_param_selection(X_train, y_train, nfolds=2) # 10 folds is too much. To per parameter\n# %%\ngrid_search.best_params_\n# %%\ngrid_search.best_estimator_\n# %%\ngrid_search.best_score_\n# %%",
        "detail": "src.models.svr.svr_model",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.models.time_series.SARIMAX.sarima_model",
        "description": "src.models.time_series.SARIMAX.sarima_model",
        "peekOfCode": "df = pd.read_csv()",
        "detail": "src.models.time_series.SARIMAX.sarima_model",
        "documentation": {}
    },
    {
        "label": "generate_data",
        "kind": 2,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "def generate_data(GW, scale, outlier_rem = False):\n    if scale is True:\n        df, std_scale_X_train, std_scale_Y_train, scl_feat_train = preprocess_data('fpl', '2019-20', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        df_test, std_scale_X_test, std_scale_Y_test, scl_feat_test  = preprocess_data('fpl', '2020-21', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        X_train, y_train, X_test, y_test = split_data(df, df_test, GW)\n        return X_train, y_train, X_test, y_test, std_scale_X_train, std_scale_Y_train, scl_feat_train, std_scale_X_test, std_scale_Y_test, scl_feat_test\n    elif scale is False:\n        df = preprocess_data('fpl', '2019-20', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        df_test = preprocess_data('fpl', '2020-21', OHE = True, FEAT = True, OUTLIER = outlier_rem, SCL = scale)\n        X_train, y_train, X_test, y_test = split_data(df, df_test, GW)",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "regression_results",
        "kind": 2,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "def regression_results(y_true, y_pred, model_string):\n    # Regression metrics\n    print('# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>')\n    print(model_string)\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n    print('explained_variance: ', round(explained_variance,4))    ",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "def plot_results(true_value, predicted_value, title):\n    plt.figure(figsize=(10,10))\n    plt.scatter(true_value, predicted_value, c='crimson')\n    p1 = max(max(predicted_value), max(true_value))\n    p2 = min(min(predicted_value), min(true_value))\n    plt.plot([p1, p2], [p1, p2], 'b-')\n    plt.xlabel('True Values', fontsize=15)\n    plt.ylabel('Predictions', fontsize=15)\n    plt.title(title)\n    plt.axis('equal')",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "rand_rand_search",
        "kind": 2,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "def rand_rand_search():\n    X_train, y_train, X_test, y_test = generate_data(GW =1, scale = False)\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "n_estimators",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 700, num = 5)]\n# Number of features to consider at every split\nmax_features = ['sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(50, 70, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5]\n# Minimum number of samples required at each leaf node\n# Method of selecting samples for training each tree",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "max_features",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "max_features = ['sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(50, 70, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5]\n# Minimum number of samples required at each leaf node\n# Method of selecting samples for training each tree\nbootstrap = [False]\n# Create the random grid",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "max_depth",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "max_depth = [int(x) for x in np.linspace(50, 70, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5]\n# Minimum number of samples required at each leaf node\n# Method of selecting samples for training each tree\nbootstrap = [False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n            'max_features': max_features,",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "min_samples_split",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "min_samples_split = [5]\n# Minimum number of samples required at each leaf node\n# Method of selecting samples for training each tree\nbootstrap = [False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'bootstrap': bootstrap}",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "bootstrap",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "bootstrap = [False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'bootstrap': bootstrap}\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "random_grid",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "random_grid = {'n_estimators': n_estimators,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'bootstrap': bootstrap}\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "rf",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "rf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_grid = GridSearchCV(estimator = rf, param_grid= random_grid, cv=3, verbose=2)\n# Fit the random search model\nrf_grid.fit(X_train, y_train) # 12h00\nrf_grid.best_estimator_\n# %%\nregressor = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',\n                    min_samples_split=5, n_estimators=600)",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "rf_grid",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "rf_grid = GridSearchCV(estimator = rf, param_grid= random_grid, cv=3, verbose=2)\n# Fit the random search model\nrf_grid.fit(X_train, y_train) # 12h00\nrf_grid.best_estimator_\n# %%\nregressor = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',\n                    min_samples_split=5, n_estimators=600)\nregressor.fit(X_train, y_train)\ny_pred_test = regressor.predict(X_test)\ny_pred_test = np.round(y_pred_test) # NB: Rounding",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "regressor",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "regressor = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',\n                    min_samples_split=5, n_estimators=600)\nregressor.fit(X_train, y_train)\ny_pred_test = regressor.predict(X_test)\ny_pred_test = np.round(y_pred_test) # NB: Rounding\ny_pred_train = regressor.predict(X_train)\ny_pred_train = np.round(y_pred_train) # NB: Rounding\n# %%\nregression_results(y_test, y_pred_test, 'RF')\n# %%",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "y_pred_test",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "y_pred_test = regressor.predict(X_test)\ny_pred_test = np.round(y_pred_test) # NB: Rounding\ny_pred_train = regressor.predict(X_train)\ny_pred_train = np.round(y_pred_train) # NB: Rounding\n# %%\nregression_results(y_test, y_pred_test, 'RF')\n# %%",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "y_pred_test",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "y_pred_test = np.round(y_pred_test) # NB: Rounding\ny_pred_train = regressor.predict(X_train)\ny_pred_train = np.round(y_pred_train) # NB: Rounding\n# %%\nregression_results(y_test, y_pred_test, 'RF')\n# %%",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "y_pred_train",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "y_pred_train = regressor.predict(X_train)\ny_pred_train = np.round(y_pred_train) # NB: Rounding\n# %%\nregression_results(y_test, y_pred_test, 'RF')\n# %%",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "y_pred_train",
        "kind": 5,
        "importPath": "src.models.trees.rf_model",
        "description": "src.models.trees.rf_model",
        "peekOfCode": "y_pred_train = np.round(y_pred_train) # NB: Rounding\n# %%\nregression_results(y_test, y_pred_test, 'RF')\n# %%",
        "detail": "src.models.trees.rf_model",
        "documentation": {}
    },
    {
        "label": "Access_URL",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def Access_URL(url):\n    \"\"\"[\n        This function simply sends a GET request to the URL]\n    Args:\n        url ([type]): [The URL to access]\n    Returns:\n        [JSON]: [Returns the json-encoded content of the response]\n    \"\"\"    \n    r = requests.get(url) \n    json = r.json() ",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "Get_FPL_Data",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def Get_FPL_Data(path):\n    \"\"\"[\n        This function collects the relevant current season's data from the Bootstrap API and writes them to CSV files\n        players_raw.csv - Current Season Statistics per Player\n        players_type.csv - Team Constraints and Conversion For Element_Types in Elements_DF\n        teams.csv - Team codes and statistics]\n    Args:\n        path ([type]): [The location where to save the resulting CSV file]\n    \"\"\"    \n    json = Access_URL(url = 'https://fantasy.premierleague.com/api/bootstrap-static/')",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "Get_Player_Historic_Data",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def Get_Player_Historic_Data(data_path, player_history_path):\n    \"\"\"[\n        This function writes a CSV file for each player which contains \n        some averaged statistics regarding the players past season histories [/history.csv] [2015/16 - 2019/20].\n        The functions also writes a CSV file for the players' current season gameweek history [/gw.csv] [2020/2021]\n    Args:\n        player_history_path ([type]): [The directory of the data]\n        players_data ([type]): [The elements_df/players_raw.csv from Get_FPL_Data]\n    Inspiration for this approach:\n        @github.com/ritviyer/",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "get_teams",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def get_teams(directory):\n    \"\"\"[This function returns FPL team data]\n    Args:\n        directory ([type]): [Where the FPL CSV is written]\n    Credit:\n        https://github.com/vaastav/Fantasy-Premier-League/blob/master/collector.py\n    \"\"\"    \n    teams = {}\n    fin = open(directory + \"/teams.csv\", 'rU')\n    reader = csv.DictReader(fin)",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "get_fixtures",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def get_fixtures(directory):\n    \"\"\"[This function returns FPL fixture data]\n    Args:\n        directory ([type]): [Where the FPL CSV is written]\n    Credit:\n        https://github.com/vaastav/Fantasy-Premier-League/blob/master/collector.py\n    \"\"\"    \n    fixtures_home = {}\n    fixtures_away = {}\n    fin = open(directory + \"/fixtures.csv\", 'rU')",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "get_positions",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def get_positions(directory):\n    \"\"\"[This function returns FPL position data]\n    Args:\n        directory ([type]): [Where the FPL CSV is written]\n    Credit:\n        https://github.com/vaastav/Fantasy-Premier-League/blob/master/collector.py\n    \"\"\"    \n    positions = {}\n    names = {}\n    pos_dict = {'1': \"GK\", '2': \"DEF\", '3': \"MID\", '4': \"FWD\"}",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "collect_gw",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def collect_gw(gw, gameweek_path, data_path, player_path):\n    \"\"\"[This function scrapes the FPL data and collects data for a ]\n    Args:\n        directory ([type]): [Where the FPL CSV is written]\n    Credit:\n        https://github.com/vaastav/Fantasy-Premier-League/blob/master/collector.py\n    \"\"\"    \n    rows = []\n    fieldnames = []\n    fixtures_home, fixtures_away = get_fixtures(data_path)",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "collect_all_gw",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def collect_all_gw(season, gameweek_path, data_path, player_path):\n    \"\"\"[This function recursively calls collect all_gw]\n    Args:\n        max_gw ([type]): [description]\n        gameweek_path ([type]): [description]\n        data_path ([type]): [description]\n        player_path ([type]): [description]\n    \"\"\"    \n    if season == '2019-20':\n        max_gw = 47",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "count_directory",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def count_directory(gws_folder):\n    \"\"\"[This function lists the number of gameweek files that can be merged within a directory]\n    Args:\n        path ([type]): [The folder where the gameweeks are ]\n    Returns:\n        [type]: [The number of gameweeek files to merge]\n    \"\"\"\n    count_num_gw = 0\n    for file in os.listdir(gws_folder):\n        if file.startswith('gw'): # eg: '.txt'",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "delete_directory",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def delete_directory(gws_folder):\n    \"\"\"[This function is a hard-coded solution to a read-in error within the 2019 data, where gaemweeks 30 - 38 contain no data, yet 39 - 47 do.]\n    Args:\n        gws_folder ([type]): [description]\n    \"\"\"    \n    if os.path.exists(gws_folder + 'gw47.csv'):\n        for gw in range(30, 39):\n            os.remove(gws_folder + f'gw{gw}.csv')\n            os.rename(src=gws_folder + f'gw{gw+9}.csv',\n                    dst=gws_folder + f'gw{gw}.csv')",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "merge_gw",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def merge_gw(type, gameweek_path):  \n    \"\"\"[This function scans through the directory of different gameweek histories, and merges them \n    into one file]\n    Args:\n        gameweek_path ([type]): [Where the gameweek files are listed]\n        type: 'FPL' or 'Understat'\n    Inspiration:\n        https://github.com/vaastav/Fantasy-Premier-League/blob/master/collector.py\n    \"\"\" \n    count_directory(gameweek_path)",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "write_league_players",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def write_league_players(understat_path, season):\n    \"\"\"[This function calls get_leauge_players and writes the resulting file to a csv]\n    Args:\n        understat_path ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    loop = asyncio.get_event_loop()\n    players = loop.run_until_complete(get_league_players(season))\n    player = pd.DataFrame.from_dict(players) # Equivalent of players_raw.csv",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "get_all_player_history",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def get_all_player_history(understat_path, season):\n    \"\"\"[This function extracts the understat data and writes it as a csv]\n    Args:\n        players ([type]): [The result of write_league_players]\n    \"\"\"    \n    start_date, end_date = set_season_time(season)\n    players = write_league_players(understat_path, season) # get all league players\n    for i in range(len(players)):\n        loop = asyncio.get_event_loop() \n        result = loop.run_until_complete(get_player_history(int(players.loc[i][0])))",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "def main(season):\n    # //: The data obtained from vaastav needs to be copied before starting this \n    # Updt 1: 2021/06/30 - Clean up code a little, untested\n    data_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//'\n    player_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//players//'\n    gameweek_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//gws//' \n    understat_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//understat//'\n    print(f'General data location: {data_path}', end = '\\n')\n    if season == '2020-21': \n        inp_general = input('Delete existing general data and download again? [y/n]\\n')",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "pbar",
        "kind": 5,
        "importPath": "src.1a_collect_data",
        "description": "src.1a_collect_data",
        "peekOfCode": "pbar = pb.ProgressBar()\ndef Access_URL(url):\n    \"\"\"[\n        This function simply sends a GET request to the URL]\n    Args:\n        url ([type]): [The URL to access]\n    Returns:\n        [JSON]: [Returns the json-encoded content of the response]\n    \"\"\"    \n    r = requests.get(url) ",
        "detail": "src.1a_collect_data",
        "documentation": {}
    },
    {
        "label": "merge_fixtures",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def merge_fixtures(fpl_path, understat_path, data_path):\n    \"\"\"[Merges team and fixtures onto fpl. Slightly processes data.]\n    Args:\n        fpl_path ([type]): [description]\n        understat_path ([type]): [description]\n        data_path ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"\n    understat = pd.read_csv(understat_path + 'all_understat_players.csv')",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "rename_teams",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def rename_teams(teams, fpl, cols = ['team', 'opponent_team', 'team_a', 'team_h']):\n    \"\"\"[This function replaces the integer value used to uniquely identify teams \n    with the FPL's associated string; 1 -> Manchester United, 2 -> Manchester City]\n    Args:\n        teams ([type]): [description]\n        fpl ([type]): [description]\n        cols (list, optional): [description]. Defaults to ['team','opponent_team', 'team_a', 'team_h'].\n    Returns:\n        [type]: [A converted dataframe]\n    \"\"\"",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "intersect",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def intersect(a, b):\n    \"\"\"[This function finds the intersection between two player name columns]\n    Args:\n        a ([type]): [description]\n        b ([type]): [description]\n    Returns:\n        [type]: [The intersection]\n    \"\"\"    \n    # print(len(list(set(a) & set(b))), 'unique and matching names between FPL and Understat')\n    return list(set(a) & set(b))",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "rename_fpl_teams",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def rename_fpl_teams(fpl, features = ['team', 'team_a', 'team_h']):\n    \"\"\"[This function replaces the acronyms used to indicate teams by the FPL API with the teams full names, as seen in the understat data]\n    Args:\n        fpl ([type]): [description]\n    Returns:\n        [type]: [A renamed dataframe]\n    NOTE:\n        New teams from different seasons need to be added here\n    \"\"\"    \n    team_reps = {",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "get_matching_names",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def get_matching_names(understat_names, fpl_names):\n    \"\"\"[This function checks for similarity between understat and fpl names, and returns a dataframe with all the unique understat names with the most similarly \n    matching FPL name.]\n    Args:\n        understat_names ([type]): [description]\n        fpl_names ([type]): [description]\n        threshold ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    ",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "exact_matches",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def exact_matches(understat, fpl, join = 'inner'):\n    \"\"\"[This function performs the first initial match, that is the entries whos names and dates match exactly between the\n    understat and fpl datasets]\n    Args:\n        understat ([type]): [description]\n        fpl ([type]): [description]\n    Returns:\n        [type]: [The merged data, and the sets of data used to construct it]\n    \"\"\"    \n    matching_names = intersect(understat['player_name'].unique(),fpl['player_name'].unique())",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "remove_matched_names",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def remove_matched_names(fpl, understat, exact_merge):\n    \"\"\"[This function checks which names were matched in the first name/date match performed, removes them and returns the \n    entries who were not matched]\n    Args:\n        fpl ([type]): [description]\n        understat ([type]): [description]\n        exact_merge ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    ",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "map_similar_names",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def map_similar_names(similarity_matched_df, understat_not_matched, fpl_not_matched, season):\n    \"\"\"[This function performs the second match, by usin the similarity dataframe returned from get_matching_names. It is manually encoded to identify wrongly mapped names\n    from each season which are removed from the dataset. The remaining understat names are used to rename the understat datasets player names\n    and acronyms to the most logical and similar matching names]\n    Args:\n        similarity_matched_df ([type]): [description]\n        understat_not_matched ([type]): [description]\n        fpl_not_matched ([type]): [description]\n    NOTE:\n        New names from different seasons need to be added here",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "final_rename",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def final_rename(understat_no_similar, fpl_no_similar, join = 'inner'):\n    \"\"\"[This function performs the third and final manual matching. It manually investigates those names that had no similar name, searches for the player name\n    or nickname in the understat data, checks the team, Googles the player's true name, and finds the corresponding FPL name. The function then renames all those \n    understat entries to the associated FPL name]\n    Args:\n        understat_no_similar ([type]): [description]\n        fpl_no_similar ([type]): [description]\n    Returns:\n        [type]: [description]\n    NOTE:",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "final_merge_understat",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def final_merge_understat(exact_merge, similar_merge, manual_merge, understat):\n    \"\"\"[This function merges the three matches performed]\n    Args:\n        exact_merge ([type]): [description]\n        similar_merge ([type]): [description]\n        manual_merge ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    understat_final = pd.concat([exact_merge, similar_merge, manual_merge])",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "join_data",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def join_data(fpl, understat, season):\n    exact_merge = exact_matches(understat, fpl) # Data merged on player name and match date\n    fpl_not_matched, understat_not_matched = remove_matched_names(fpl, understat, exact_merge) # Those names that did not match previously\n    similarity_matched_df = get_matching_names(understat_not_matched, fpl_not_matched) \n    understat_no_similar, understat_similar, fpl_similar, fpl_no_similar = map_similar_names(similarity_matched_df, understat_not_matched, fpl_not_matched, season)  # Note: Manual investigation\n    similar_merge = pd.merge(fpl_similar, understat_similar, left_on=['player_name', 'kickoff_time'], right_on=['player_name', 'date']) \n    no_similar_matches_df = get_matching_names(understat_no_similar, fpl_no_similar) # Note: Manual investigation\n    manual_merge = final_rename(understat_no_similar, fpl_no_similar)\n    understat_final = final_merge_understat(exact_merge, similar_merge, manual_merge, understat)\n    return fpl, understat_final",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "def main(season):\n    fpl_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//gws//' \n    understat_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//understat//'\n    data_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//'\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    understat, fpl = merge_fixtures(fpl_path, understat_path, data_path)\n    fpl, understat = join_data(fpl, understat, season)\n    fpl.to_csv(training_path + 'fpl.csv', index = False)\n    understat.to_csv(training_path + 'understat_merged.csv', index = False)\nif __name__ == \"__main__\":",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "pd.options.mode.chained_assignment",
        "kind": 5,
        "importPath": "src.1b_merge_data",
        "description": "src.1b_merge_data",
        "peekOfCode": "pd.options.mode.chained_assignment = None\ndef merge_fixtures(fpl_path, understat_path, data_path):\n    \"\"\"[Merges team and fixtures onto fpl. Slightly processes data.]\n    Args:\n        fpl_path ([type]): [description]\n        understat_path ([type]): [description]\n        data_path ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"",
        "detail": "src.1b_merge_data",
        "documentation": {}
    },
    {
        "label": "drop_dups",
        "kind": 2,
        "importPath": "src.1c_process_data",
        "description": "src.1c_process_data",
        "peekOfCode": "def drop_dups(df):\n    \"\"\"[Drop duplicated columns from the merging process]\n    Args:\n        df ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    init_len = len(df.columns)\n    df.drop([col for col in df.columns if col.endswith('_y')], axis=1, inplace=True) \n    df.columns = df.columns.str.replace('_x', '')",
        "detail": "src.1c_process_data",
        "documentation": {}
    },
    {
        "label": "add_season",
        "kind": 2,
        "importPath": "src.1c_process_data",
        "description": "src.1c_process_data",
        "peekOfCode": "def add_season(df, season):\n    \"\"\"[Assigns a constant season for easier referencing]\n    Args:\n        df ([type]): [description]\n        season ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    if season == '2019-20':\n        df['season'] = 2019",
        "detail": "src.1c_process_data",
        "documentation": {}
    },
    {
        "label": "merge_seasons",
        "kind": 2,
        "importPath": "src.1c_process_data",
        "description": "src.1c_process_data",
        "peekOfCode": "def merge_seasons(sort_by):\n    season = '2019-20'\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    df_1 = pd.read_csv(training_path + 'cleaned_fpl.csv')\n    df_3= pd.read_csv(training_path + 'cleaned_understat.csv')\n    season='2020-21'\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    df_2 = pd.read_csv(training_path + 'cleaned_fpl.csv')\n    df_4 = pd.read_csv(training_path + 'cleaned_understat.csv')\n    fpl_cols=['player_name','team','position','value','minutes','bps','GW','kickoff_time','season','total_points','creativity','influence','threat','ict_index','assists','bonus','goals_conceded','goals_scored','saves','own_goals','penalties_saved','penalties_missed','red_cards','yellow_cards','team_h_score','team_a_score','clean_sheets','was_home','opponent_team','selected','transfers_in','transfers_out','transfers_balance','strength_attack_away','strength_attack_home','strength_defence_away','strength_defence_home','strength_overall_away','strength_overall_home']",
        "detail": "src.1c_process_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.1c_process_data",
        "description": "src.1c_process_data",
        "peekOfCode": "def main(season):\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    fpl = pd.read_csv(training_path + 'fpl.csv').pipe(drop_dups).pipe(add_season, season)\n    fpl.sort_values(by=['GW', 'season']).to_csv(training_path + 'cleaned_fpl.csv', index = False)\n    understat = pd.read_csv(training_path + 'understat_merged.csv').pipe(drop_dups).pipe(add_season, season)\n    understat.sort_values(by=['GW', 'season']).to_csv(training_path + 'cleaned_understat.csv', index = False)\nif __name__ == \"__main__\": \n    main(season='2020-21') # Successful execution\n    main(season='2019-20') \n    merge_seasons(sort_by = ['GW', 'season', 'player_name']) ",
        "detail": "src.1c_process_data",
        "documentation": {}
    },
    {
        "label": "outliers_iqr",
        "kind": 2,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "def outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5)\n    upper_bound = quartile_3 + (iqr * 1.5)\n    return np.where((ys > upper_bound) | (ys < lower_bound))\ndef check_data_dist(df, feat = None, bins = 100, distrib = ['expon', 'norm', 'uniform']):\n    if feat is None:\n        cf, dist, sse = [], [], []\n        for feat in df.select_dtypes('number').columns:",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "check_data_dist",
        "kind": 2,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "def check_data_dist(df, feat = None, bins = 100, distrib = ['expon', 'norm', 'uniform']):\n    if feat is None:\n        cf, dist, sse = [], [], []\n        for feat in df.select_dtypes('number').columns:\n            dist_fit = Fitter(df[feat], timeout=60*10, distributions=distrib, bins=bins)\n            dist_fit.fit()\n            key, value = list(dist_fit.get_best(method = 'sumsquare_error').items())[0] # Key is the identified distribution\n            cf.append(feat), dist.append(key), sse.append(value)\n            # print(f'{feat} has {key} distribution with {value} SSE')\n        df_fitted = pd.DataFrame({'Feature': cf, 'Distribution': dist })",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_fpl.csv')\ndistrib_fit = check_data_dist(df)\ndf_2 = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_us.csv')\n# distrib_fit_2 = check_data_dist(df_2)\n# distrib_fit_2['FPL']  = distrib_fit['Distribution']\n# print(distrib_fit_2.to_latex())\n# %%\nfrom fitter import HistFit\nhf = HistFit(df['value'].to_list(), bins=100)\nhf.fit(error_rate=0.03, Nfit=20)",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "distrib_fit",
        "kind": 5,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "distrib_fit = check_data_dist(df)\ndf_2 = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_us.csv')\n# distrib_fit_2 = check_data_dist(df_2)\n# distrib_fit_2['FPL']  = distrib_fit['Distribution']\n# print(distrib_fit_2.to_latex())\n# %%\nfrom fitter import HistFit\nhf = HistFit(df['value'].to_list(), bins=100)\nhf.fit(error_rate=0.03, Nfit=20)\nprint(hf.mu, hf.sigma, hf.amplitude)",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "df_2",
        "kind": 5,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "df_2 = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_us.csv')\n# distrib_fit_2 = check_data_dist(df_2)\n# distrib_fit_2['FPL']  = distrib_fit['Distribution']\n# print(distrib_fit_2.to_latex())\n# %%\nfrom fitter import HistFit\nhf = HistFit(df['value'].to_list(), bins=100)\nhf.fit(error_rate=0.03, Nfit=20)\nprint(hf.mu, hf.sigma, hf.amplitude)\n# # %%",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "hf",
        "kind": 5,
        "importPath": "src.1d_fit_distrib",
        "description": "src.1d_fit_distrib",
        "peekOfCode": "hf = HistFit(df['value'].to_list(), bins=100)\nhf.fit(error_rate=0.03, Nfit=20)\nprint(hf.mu, hf.sigma, hf.amplitude)\n# # %%\n# # %%\n# get_common_distributions()\n# # %%\n# %%\nfrom keras.wrappers.scikit_learn import KerasRegressor",
        "detail": "src.1d_fit_distrib",
        "documentation": {}
    },
    {
        "label": "data_file",
        "kind": 5,
        "importPath": "src.2a_explore_data",
        "description": "src.2a_explore_data",
        "peekOfCode": "data_file =  'C://Users//jd-vz//Desktop//Code//data//collected_us.csv'\ndf = pd.read_csv(data_file)\ndf['SumCount'] = df.groupby(['season', 'team'])['total_points'].transform('sum')\nax=df.plot(kind='bar', x='team',y='SumCount', alpha=.7, color='g', position=1, width =.2)\ndf.plot(kind='bar', x='team',y='SumCount', ax=ax, secondary_y=True, alpha=.8, position=0, width=.2)\n# %%",
        "detail": "src.2a_explore_data",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.2a_explore_data",
        "description": "src.2a_explore_data",
        "peekOfCode": "df = pd.read_csv(data_file)\ndf['SumCount'] = df.groupby(['season', 'team'])['total_points'].transform('sum')\nax=df.plot(kind='bar', x='team',y='SumCount', alpha=.7, color='g', position=1, width =.2)\ndf.plot(kind='bar', x='team',y='SumCount', ax=ax, secondary_y=True, alpha=.8, position=0, width=.2)\n# %%",
        "detail": "src.2a_explore_data",
        "documentation": {}
    },
    {
        "label": "df['SumCount']",
        "kind": 5,
        "importPath": "src.2a_explore_data",
        "description": "src.2a_explore_data",
        "peekOfCode": "df['SumCount'] = df.groupby(['season', 'team'])['total_points'].transform('sum')\nax=df.plot(kind='bar', x='team',y='SumCount', alpha=.7, color='g', position=1, width =.2)\ndf.plot(kind='bar', x='team',y='SumCount', ax=ax, secondary_y=True, alpha=.8, position=0, width=.2)\n# %%",
        "detail": "src.2a_explore_data",
        "documentation": {}
    },
    {
        "label": "read_and_shift",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def read_and_shift():\n    \"\"\"[Returns all features that need to be shifted]\n    Args:\n        df ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"\n    non_shift = ['player_name', 'team', 'position', 'value', 'GW',\n       'kickoff_time', 'season', 'was_home', 'opponent_team', 'selected', \n       'transfers_in', 'transfers_out', 'transfers_balance', 'strength_attack_away',",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "to_cat",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def to_cat(df):\n    for col in ['season', 'clean_sheets_shift', 'own_goals_shift', 'penalties_missed_shift', 'red_cards_shift', 'yellow_cards_shift']:\n        df[col] = df[col].astype('category')\n    return df\ndef univariate_mse(df, num):\n    df = df.select_dtypes(include='number')\n    X_train, y_train = df.drop(columns=['total_points_shift']), df['total_points_shift']\n    mse_values = []\n    for feature in X_train.columns:\n        regressor = DecisionTreeRegressor()",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "univariate_mse",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def univariate_mse(df, num):\n    df = df.select_dtypes(include='number')\n    X_train, y_train = df.drop(columns=['total_points_shift']), df['total_points_shift']\n    mse_values = []\n    for feature in X_train.columns:\n        regressor = DecisionTreeRegressor()\n        gs = GridSearchCV(regressor,\n                  param_grid = {'max_depth': range(1, 11),\n                                'min_samples_split': range(10, 60, 10)},\n                  cv=5, scoring='neg_mean_squared_error')",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "rolling_avg",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def rolling_avg(data, prev_games, feats):\n    \"\"\"[This function creates a previous game rolling average for the selected features]\n    Args:\n        data ([type]): [description]\n        prev_games ([type]): [description]\n        feats ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"\n    new_feats = []",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "change_different_teams",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def change_different_teams(df):\n    changed_teams = ['Fulham', 'Leeds', 'West Brom', 'Watford',\n                     'Bournemouth', 'Norwich']  # These teams are not in both seasons\n    for feat in ['team', 'opponent_team']:\n        df[feat] = np.where(df[feat].isin(changed_teams), 'Other', df[feat])\n    return df\ndef create_team_stats(df):\n    team_stats = df[['team', 'strength_attack_home', 'strength_attack_away',\n                     'strength_defence_home', 'strength_defence_away', 'season']].drop_duplicates()\n    team_stats.rename(columns={'team': 'opponent_team', 'strength_attack_home': 'opponent_strength_attack_home',",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "create_team_stats",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def create_team_stats(df):\n    team_stats = df[['team', 'strength_attack_home', 'strength_attack_away',\n                     'strength_defence_home', 'strength_defence_away', 'season']].drop_duplicates()\n    team_stats.rename(columns={'team': 'opponent_team', 'strength_attack_home': 'opponent_strength_attack_home',\n                               'strength_attack_away': 'opponent_strength_attack_away', 'strength_defence_home': 'opponent_strength_defence_home',\n                               'strength_defence_away': 'opponent_strength_defence_away'}, inplace=True)\n    df = pd.merge(df, team_stats, on=['opponent_team', 'season'])\n    df['player_team_strength'] = np.where(df['was_home'], df['strength_attack_home'], df['strength_attack_away'])\n    df['player_team_defence'] = np.where(df['was_home'], df['strength_defence_home'], df['strength_defence_away'])\n    df['player_team_overall'] = df['player_team_strength'] / 2 + df['player_team_defence']/2",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "create_FDR",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def create_FDR(df):\n    df['FDR'] = np.where(df['opponent_team_overall'] >\n                         df['player_team_overall'], 1, -1) # High, Low\n    idx_med = np.where(df['opponent_team_overall']\n                       == df['player_team_overall'])\n    df['FDR'].iloc[idx_med] = 0 # Mid\n    df['FDR']\n    return df\ndef create_double_week(df):\n    idx = df[df.duplicated(['player_name', 'GW'], False)].index",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "create_double_week",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def create_double_week(df):\n    idx = df[df.duplicated(['player_name', 'GW'], False)].index\n    df['double_week'] = False\n    df.loc[idx, 'double_week'] = True\n    df.reset_index(drop=True, inplace=True)\n    return df\ndef one_hot_encode(df):\n    print(df.select_dtypes(exclude='number').columns)\n    ohe_cols = ['team', 'position', 'opponent_team']\n    df = pd.get_dummies(df, columns=ohe_cols, prefix=ohe_cols)",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def one_hot_encode(df):\n    print(df.select_dtypes(exclude='number').columns)\n    ohe_cols = ['team', 'position', 'opponent_team']\n    df = pd.get_dummies(df, columns=ohe_cols, prefix=ohe_cols)\n    for col in df.select_dtypes(exclude='number').columns:\n        if (len(df[col].unique()) > 2):\n            ohe_cols.append(col)\n        else:\n            df[col] = df[col].replace({df[col].unique()[0]:0, df[col].unique()[1]:1})\n    print(ohe_cols)",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "cumulative_mm",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def cumulative_mm(df):\n    df['match_num'] = 1\n    df['matches_shift_cumulative'] = df.groupby(\n        ['player_name'])['match_num'].cumsum()\n    df['minutes_shift_cumulative'] = df.groupby(\n        ['player_name'])['minutes_shift'].cumsum()\n    df.reset_index(drop=True, inplace=True)\n    df.drop(columns='match_num', inplace=True)\n    return df\ndef ratio_to_value(df, top_5):",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "ratio_to_value",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def ratio_to_value(df, top_5):\n    for col in top_5 + ['total_points_shift']:\n        df[col + '_to_value'] = df[col] / df['value']\n        df[col + '_per_minute'] = df[col] / df['minutes_shift']\n    # Since some players play 0 minutes\n    return df.replace([np.inf, -np.inf, np.nan], 0)\n# def prob_of_being_selected(df):\n#     df = df.drop_duplicates(subset=['player_name', 'GW', 'season'], keep='last')\n#     df['weekly_selection'] = df.groupby(['GW'])['selected'].transform('sum')\n#     df['prob_of_selected'] = df['selected'] / df['weekly_selection']",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "create_top_scorer",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def create_top_scorer(df):\n    df['max_points'] = df.groupby(\n        ['GW'])['total_points_shift'].transform('max')\n    df['top_scorer'] = np.where(\n        df['total_points_shift'] == df['max_points'], True, False)\n    df.drop(columns='max_points', inplace=True)\n    return df\ndef create_top_team(df):\n    df['team_points'] = df.groupby(['team', 'season', 'GW'])[\n        'total_points_shift'].transform('sum')",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "create_top_team",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def create_top_team(df):\n    df['team_points'] = df.groupby(['team', 'season', 'GW'])[\n        'total_points_shift'].transform('sum')\n    df['max_points'] = df.groupby(['season', 'GW'])['total_points_shift'].transform('max')\n    df['top_team'] = np.where(df['team_points'] == df['max_points'], True, False)\n    df.drop(columns=['max_points', 'team_points'], inplace=True)\n    return df\ndef feat_eng(df, feat_choice):\n    df['form'] = df['total_points_shift'].rolling(min_periods=1, window=4).mean().fillna(0) # * 1. Calculate 4 week form\n    # # TODO: Tune prev games to find best value",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "feat_eng",
        "kind": 2,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "def feat_eng(df, feat_choice):\n    df['form'] = df['total_points_shift'].rolling(min_periods=1, window=4).mean().fillna(0) # * 1. Calculate 4 week form\n    # # TODO: Tune prev games to find best value\n    df = df.groupby(['player_name']).apply(rolling_avg, prev_games=4, feats=feat_choice) #* 2. Associated 4 week rolling average of five highest univariate features\n    df = ratio_to_value(df, feat_choice)  # * 3. Top five + total_points to ratio to value and per minute played\n    df = create_team_stats(df) # * 4. Created opponent strength, defense and overall statistics\n    df = create_FDR(df)  # *5. Created a fixture difficulty rating of L, M, H\n    df = create_double_week(df)  # * 5. Created a binary double week\n    df = change_different_teams(df)  # * 6. Changed seasonal teams to other\n    df = create_top_team(df) # * 7. Create a binary indicator stating if a team was the highest performing team ",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "df = read_and_shift().pipe(to_cat)  # * 1: Shift the data\nfeat_choice = univariate_mse(df, 5)  # * Highest univariate decrease\nfeat_choice = ['bps_shift']\ndf = feat_eng(df, feat_choice)\ndf.to_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv', index = False)\n# %%\n# %%",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "feat_choice",
        "kind": 5,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "feat_choice = univariate_mse(df, 5)  # * Highest univariate decrease\nfeat_choice = ['bps_shift']\ndf = feat_eng(df, feat_choice)\ndf.to_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv', index = False)\n# %%\n# %%",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "feat_choice",
        "kind": 5,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "feat_choice = ['bps_shift']\ndf = feat_eng(df, feat_choice)\ndf.to_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv', index = False)\n# %%\n# %%",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.3a_feat_eng",
        "description": "src.3a_feat_eng",
        "peekOfCode": "df = feat_eng(df, feat_choice)\ndf.to_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv', index = False)\n# %%\n# %%",
        "detail": "src.3a_feat_eng",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def read_data():\n    df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv')\n    return df\ndef quasi_constant_fs(data,threshold):\n    \"\"\" detect features that show the same value for the \n    majority/all of the observations (constant/quasi-constant features)\n    Parameters\n    ----------\n    data : pd.Dataframe\n    threshold : threshold to identify the variable as constant",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "quasi_constant_fs",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def quasi_constant_fs(data,threshold):\n    \"\"\" detect features that show the same value for the \n    majority/all of the observations (constant/quasi-constant features)\n    Parameters\n    ----------\n    data : pd.Dataframe\n    threshold : threshold to identify the variable as constant\n    Returns\n    -------\n    list of variables names",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "corr_feature_detect",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def corr_feature_detect(data,threshold=0.8):\n    \"\"\" detect highly-correlated features of a Dataframe\n    Parameters\n    ----------\n    data : pd.Dataframe\n    threshold : threshold to identify the variable correlated\n    Returns\n    -------\n    pairs of correlated variables\n    \"\"\"",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "mutual_info",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def mutual_info(X, y, select_k=0):\n    if select_k == 0:\n        mi = mutual_info_regression(X,y)\n        mi = pd.Series(mi)\n        mi.index = X.columns\n        return mi.sort_values(ascending=False)\n    if select_k >= 1:\n        sel_ = SelectKBest(mutual_info_regression, k=select_k).fit(X,y)\n        col = X.columns[sel_.get_support()]\n        return col",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "calculate_vif",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def calculate_vif(data):\n    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])\n    x_var_names = data.columns\n    for i in range(0, x_var_names.shape[0]):\n        y = data[x_var_names[i]]\n        x = data[x_var_names.drop([x_var_names[i]])]\n        r_squared = sm.OLS(y,x).fit().rsquared\n        vif = round(1/(1-r_squared),2)\n        vif_df.loc[i] = [x_var_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "univariate_mse",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def univariate_mse(df,threshold):\n    \"\"\"\n    First, it builds one decision tree per feature, to predict the target\n    Second, it makes predictions using the decision tree and the mentioned feature\n    Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n    It selects the highest ranked features\n    \"\"\"\n    df = df.select_dtypes(include='number') # NB: Only includes numeric\n    X_train, X_test, y_train, y_test= train_test_split(df.drop(columns = ['total_points_shift']), df['total_points_shift'], test_size=0.2) \n    mse_values = []",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "boruta_fs",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def boruta_fs(X_train, y_train):\n    # The important point is for BorutaPy, multicollinearity should be removed before running it.\n    forest = RandomForestRegressor()\n    forest.fit(X_train, y_train)\n    feat_selector = BorutaPy(forest, verbose=2, random_state=1,max_iter = 15)\n    feat_selector.fit(np.array(X_train), np.array(y_train))   # find all relevant features\n    feat_selector.support_ # check selected features\n    feat_selector.ranking_ # check ranking of features\n    X_filtered = feat_selector.transform(np.array(X_train))    # call transform() on X to filter it down to selected features\n    # zip my names, ranks, and decisions in a single iterable",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "write_log",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def write_log(iterations_log, data_str, elim_crit, direction): \n    log_dir = 'C://Users//jd-vz//Desktop//Code//src//features//'\n    iterations_file = open(log_dir + f'{data_str}_{elim_crit}_{direction}_select_logs.txt',\"w+\") \n    iterations_file.write(iterations_log)\n    iterations_file.close()\ndef backward_elimination(X, y, data_str, threshold_in = 0.05, verbose = True):\n    \"\"\"[This function recursively eliminates features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "backward_elimination",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def backward_elimination(X, y, data_str, threshold_in = 0.05, verbose = True):\n    \"\"\"[This function recursively eliminates features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]\n        verbose (bool, optional): [description]. Defaults to True.\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    iterations_log = \"\"",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "forward_elimination",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def forward_elimination(X, y, data_str, threshold_in = 0.05, verbose=True):\n    \"\"\"[This function recursively adds features and returns all features that are above the level of significance]\n    Args:\n        df ([type]): [description]\n        threshold_in ([type]): [description]\n        verbose (bool, optional): [description]. Defaults to True.\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    initial_list = []",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "stepwise_selection",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n    initial_features = data.columns.tolist()\n    best_features = []\n    while (len(initial_features)>0):\n        remaining_features = list(set(initial_features)-set(best_features))\n        new_pval = pd.Series(index=remaining_features)\n        for new_column in remaining_features:\n            # print('Adding ', new_column)\n            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n            new_pval[new_column] = model.pvalues[new_column]",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "mi_mrmr",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def mi_mrmr(X_train, y_train, X_test, y_test, max_feat = 100, min_feat = 10, step_size = 5, algos = ['mrmr', 'mi']):\n    ranking = pd.DataFrame(index = range(X_train.shape[1]))\n    mrmr  = mrmr_regression(X_train, y_train, K = X_train.shape[1])\n    ranking['mrmr'] = pd.Series([X_train.columns.get_loc(c) for c in mrmr], index = ranking.index)\n    mi = mutual_info(X_train, y_train)\n    ranking['mi'] = pd.Series([X_train.columns.get_loc(c) for c in mi.index], index = ranking.index)\n    # f = f_regression(X_train, y_train)[0]\n    # ranking['f'] = pd.Series([X_train.columns.get_loc(c) for c in mi.index], index = ranking.index)\n    ks = range(min_feat, max_feat, step_size)\n    loss = pd.DataFrame(index = ks, columns = algos)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "to_cat",
        "kind": 2,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "def to_cat(df):\n    for col in ['season', 'clean_sheets_shift', 'own_goals_shift', 'penalties_missed_shift', 'red_cards_shift', 'yellow_cards_shift']:\n        df[col] = df[col].astype('category')\n    return df\ndf = read_data()\nX_train, y_train = df.drop(['total_points_shift', 'kickoff_time', 'player_name'], axis = 1), df['total_points_shift']\nstepwise_selection(X_train, y_train)\n# %%\nX_filtered = boruta_fs(X_train, y_train) \nforward_elimination(X_train, y_train, 'us')",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "df = read_data()\nX_train, y_train = df.drop(['total_points_shift', 'kickoff_time', 'player_name'], axis = 1), df['total_points_shift']\nstepwise_selection(X_train, y_train)\n# %%\nX_filtered = boruta_fs(X_train, y_train) \nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%\nquasi_constant_features = quasi_constant_fs(df , threshold=0.99) # [['penalties_saved_shift', 'penalties_missed_shift', 'own_goals_shift', 'red_cards_shift']]\ncorr_feature_detect(df, threshold=0.9)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "X_filtered",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "X_filtered = boruta_fs(X_train, y_train) \nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%\nquasi_constant_features = quasi_constant_fs(df , threshold=0.99) # [['penalties_saved_shift', 'penalties_missed_shift', 'own_goals_shift', 'red_cards_shift']]\ncorr_feature_detect(df, threshold=0.9)\nunivariate_mse(df, threshold=7)\n# %%\nvif_param = calculate_vif(df.select_dtypes(include='number').drop('total_points_shift', axis = 1))\nprint(vif_param[vif_param['Vif'] < 15]['Var'].values.tolist())",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "quasi_constant_features",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "quasi_constant_features = quasi_constant_fs(df , threshold=0.99) # [['penalties_saved_shift', 'penalties_missed_shift', 'own_goals_shift', 'red_cards_shift']]\ncorr_feature_detect(df, threshold=0.9)\nunivariate_mse(df, threshold=7)\n# %%\nvif_param = calculate_vif(df.select_dtypes(include='number').drop('total_points_shift', axis = 1))\nprint(vif_param[vif_param['Vif'] < 15]['Var'].values.tolist())\n# %%\n    #Compute VIF data for each independent variable\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "vif_param",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "vif_param = calculate_vif(df.select_dtypes(include='number').drop('total_points_shift', axis = 1))\nprint(vif_param[vif_param['Vif'] < 15]['Var'].values.tolist())\n# %%\n    #Compute VIF data for each independent variable\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"features\"] = df.select_dtypes(include='number').drop('total_points_shift', axis = 1).columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).values, i) for i in range(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).shape[1])]\nvif\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "vif",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "vif = pd.DataFrame()\nvif[\"features\"] = df.select_dtypes(include='number').drop('total_points_shift', axis = 1).columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).values, i) for i in range(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).shape[1])]\nvif\n# %%\nX_train, y_train = df.drop(['total_points_shift', 'kickoff_time', 'player_name'], axis = 1), df['total_points_shift']\nX_filtered = boruta_fs(X_train, y_train) # generate x_train, y_train at boruta_fs\nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "vif[\"features\"]",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "vif[\"features\"] = df.select_dtypes(include='number').drop('total_points_shift', axis = 1).columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).values, i) for i in range(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).shape[1])]\nvif\n# %%\nX_train, y_train = df.drop(['total_points_shift', 'kickoff_time', 'player_name'], axis = 1), df['total_points_shift']\nX_filtered = boruta_fs(X_train, y_train) # generate x_train, y_train at boruta_fs\nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%\nranking, loss = mi_mrmr(X_train, y_train, X_test, y_test)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "vif[\"vif_Factor\"]",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "vif[\"vif_Factor\"] = [variance_inflation_factor(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).values, i) for i in range(df.select_dtypes(include='number').drop('total_points_shift', axis = 1).shape[1])]\nvif\n# %%\nX_train, y_train = df.drop(['total_points_shift', 'kickoff_time', 'player_name'], axis = 1), df['total_points_shift']\nX_filtered = boruta_fs(X_train, y_train) # generate x_train, y_train at boruta_fs\nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%\nranking, loss = mi_mrmr(X_train, y_train, X_test, y_test)\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "X_filtered",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "X_filtered = boruta_fs(X_train, y_train) # generate x_train, y_train at boruta_fs\nforward_elimination(X_train, y_train, 'us')\nbackward_elimination(X_train, y_train, 'us')\n# %%\nranking, loss = mi_mrmr(X_train, y_train, X_test, y_test)\n# %%\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "featureSelector",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "featureSelector = SelectKBest(score_func=f_regression,k=X_train.shape[1])\nfeatureSelector.fit(X_train,y_train)\nprint([zero_based_index for zero_based_index in list(featureSelector.get_support(indices=True))])\n# %%\n[X_train.columns.get_loc(c) for c in list(featureSelector.get_support(indices=True))]\n# %%\n# Create an SelectKBest object to select features with two best ANOVA F-Values\nfvalue_selector = SelectKBest(f_regression, k=2)\n# Apply the SelectKBest object to the features and target\nX_kbest = fvalue_selector.fit_transform(X_train, y_train)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "fvalue_selector",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "fvalue_selector = SelectKBest(f_regression, k=2)\n# Apply the SelectKBest object to the features and target\nX_kbest = fvalue_selector.fit_transform(X_train, y_train)\n# Show results\nprint('Original number of features:', X_train.shape[1])\nprint('Reduced number of features:', X_kbest.shape[1])\n# %%\n# %%\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "X_kbest",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "X_kbest = fvalue_selector.fit_transform(X_train, y_train)\n# Show results\nprint('Original number of features:', X_train.shape[1])\nprint('Reduced number of features:', X_kbest.shape[1])\n# %%\n# %%\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest\nfor alph in [0.0001, 0.001, 0.01, 0.1]:\n    print(alph)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "pipeline = Pipeline([\n                     ('scaler',StandardScaler()),\n                     ('model',ElasticNet())\n])\nsearch = GridSearchCV(pipeline, {'alpha':np.arange(1e-5,1e-3,1e-1), \n                                 'l1_ratio':np.arange(0, 1, 0.01)},\n                      cv = 5, scoring=\"neg_mean_squared_error\",\n                      verbose=3)\nsearch.fit(X_train,y_train)\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "search = GridSearchCV(pipeline, {'alpha':np.arange(1e-5,1e-3,1e-1), \n                                 'l1_ratio':np.arange(0, 1, 0.01)},\n                      cv = 5, scoring=\"neg_mean_squared_error\",\n                      verbose=3)\nsearch.fit(X_train,y_train)\n# %%\n# %%\nprint(search.best_params_)\ncoefficients = search.best_estimator_.named_steps['model'].coef_\nimportance = np.abs(coefficients)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "coefficients = search.best_estimator_.named_steps['model'].coef_\nimportance = np.abs(coefficients)\n# %%\n# %%\nprint(search.best_params_)\n# %%\nnp.array(X_train.columns)[importance > 0]\n# %%\nnp.array(X_train.columns)[importance == 0]\n# %%",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "importance",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "importance = np.abs(coefficients)\n# %%\n# %%\nprint(search.best_params_)\n# %%\nnp.array(X_train.columns)[importance > 0]\n# %%\nnp.array(X_train.columns)[importance == 0]\n# %%\nprint(coefficients)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "model = ElasticNet(alpha = 0, l1_ratio=0)\nmodel.fit(X_train, y_train)\n# %%\nmodel.coef_\n# %%\n# define model\nmodel = ElasticNet()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "model = ElasticNet()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\ngrid['l1_ratio'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "cv",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\ngrid['l1_ratio'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "grid",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "grid = dict()\ngrid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\ngrid['l1_ratio'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "grid['alpha']",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "grid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\ngrid['l1_ratio'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n# def collect_features(be_t, fe_t):",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "grid['l1_ratio']",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "grid['l1_ratio'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n# def collect_features(be_t, fe_t):\n#     lst = be_t +  fe_t",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n# def collect_features(be_t, fe_t):\n#     lst = be_t +  fe_t\n#     selected = pd.DataFrame(Counter(lst).most_common())\n#     selected = selected[selected[0] != 'intercept']",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "src.3b_select_data",
        "description": "src.3b_select_data",
        "peekOfCode": "results = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n# def collect_features(be_t, fe_t):\n#     lst = be_t +  fe_t\n#     selected = pd.DataFrame(Counter(lst).most_common())\n#     selected = selected[selected[0] != 'intercept']\n#     selected.rename(columns = {0:'Feature', 1:'Occurences'}, inplace = True)\n#     return selected",
        "detail": "src.3b_select_data",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def read_data():\n    shift = []\n    # df_test = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//shifted_us.csv')\n    # df_train = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//shifted_us.csv')\n    df_test = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//training//shifted_fpl.csv')\n    df_train = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//shifted_fpl.csv')\n    shifted_feats = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//features//shifted.csv')['features']\n    for df in df_train, df_test:\n        for feat in shifted_feats:\n            if feat in df.columns:",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "change_different_teams",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def change_different_teams(df_test, df_train):\n    changed_teams = ['Fulham', 'Leeds', 'West Brom', 'Watford', 'Bournemouth', 'Norwich'] # These teams are not in both seasons\n    for df in df_test, df_train:\n        for feat in ['team', 'opponent_team', 'team_a', 'team_h']:\n            df.loc[df[feat].isin(changed_teams), feat] = 'Other'\n            df[feat].astype(\"category\")\n    return df_test, df_train\ndef one_hot_encode(df_test):\n    \"\"\"[One hot encode all features but the players name and the kickoff time]\n    Args:",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def one_hot_encode(df_test):\n    \"\"\"[One hot encode all features but the players name and the kickoff time]\n    Args:\n        df ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"\n    cat = df_test.select_dtypes(exclude='number').columns.drop(['player_name', 'kickoff_time']) \n    df_test = pd.get_dummies(df_test, columns=cat, prefix=cat)\n    return df_test",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def plot_results(true_value, predicted_value, title):\n    plt.figure(figsize=(10,10))\n    plt.scatter(true_value, predicted_value, c='crimson')\n    p1 = max(max(predicted_value), max(true_value))\n    p2 = min(min(predicted_value), min(true_value))\n    plt.plot([p1, p2], [p1, p2], 'b-')\n    plt.xlabel('True Values', fontsize=15)\n    plt.ylabel('Predictions', fontsize=15)\n    plt.title(title)\n    plt.axis('equal')",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "regression_results",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def regression_results(y_true, y_pred, model_string):\n    print('# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>')\n    print(model_string)\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    r2=metrics.r2_score(y_true, y_pred)\n    print('explained_variance: ', round(explained_variance,4))    \n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "scaled_metrics",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def scaled_metrics(regressor, X_train, y_train, X_test, y_test, scl_y, GW, model_str, epochs = None):\n    if model_str is 'Scaled Net':\n        regressor.fit(X_train, y_train, epochs=50)\n    else:\n        regressor.fit(X_train, y_train)\n        if model_str == 'Scaled RF':\n            plot_feature_importance(regressor.feature_importances_, X_train.columns,'RANDOM FOREST')\n    y_train_rescaled = np.round(scl_y.inverse_transform(y_train))\n    y_test_rescaled = np.round(scl_y.inverse_transform(y_test))\n    y_pred_test = np.round(scl_y.inverse_transform(regressor.predict(X_test)))",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "test_scaled_LR_model",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def test_scaled_LR_model(X_train, y_train, X_test, y_test, scl_y, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = LinearRegression(normalize=False, n_jobs=-1)\n    scaled_metrics(regressor, X_train, y_train, X_test, y_test, scl_y, GW, 'Scaled LR')",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "test_scaled_knn_model",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def test_scaled_knn_model(X_train, y_train, X_test, y_test, scl_y, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    for k in [1, 3, 5, 7]:\n        print('k = ', k, end = '\\n')",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "plot_feature_importance",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def plot_feature_importance(importance,names,model_type):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    #Define size of bar plot",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "test_scaled_RF_model",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scale_Y, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = RandomForestRegressor()  \n    scaled_metrics(regressor, X_train, y_train, X_test, y_test, std_scale_Y, GW, 'Scaled RF')",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "test_scaled_SVR_model",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def test_scaled_SVR_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW):\n    \"\"\"[This function tests a linear regression model on the provided data]\n    Args:\n        X_train ([type]): [The training predictors]\n        X_test ([type]): [The testing predictors]\n        y_train ([type]): [The testing predictors]\n        y_test ([type]): [The training response]\n    \"\"\"\n    regressor = SVR(verbose=True, kernel='linear') # Note: This was fitted to scaled data\n    scaled_metrics(regressor, X_train, y_train, X_test, y_test, std_scaler_Y, GW, 'Scaled SVR')",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "plot_history",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Square Error')\n    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n    plt.legend()\n    plt.ylim([0,0.05])",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "test_scaled_net",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def test_scaled_net(X_train, y_train, X_test, y_test, std_scale_Y, GW):\n    \"\"\"[This is very much just a]\n    Args:\n        X_train ([type]): [description]\n        y_train ([type]): [description]\n        X_test ([type]): [description]\n        y_test ([type]): [description]\n        std_scale_Y_train ([type]): [description]\n        std_scale_Y_test ([type]): [description]\n        GW ([type]): [description]",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "reduce_multicollin",
        "kind": 2,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "def reduce_multicollin(X_train, y_train, X_test, y_test):\n    pca = PCA(n_components = X_train.shape[1])\n    pca_data = pca.fit_transform(X_train)\n    percent_var_explained = pca.explained_variance_/(np.sum(pca.explained_variance_))\n    cumm_var_explained = np.cumsum(percent_var_explained)\n    plt.plot(cumm_var_explained)\n    plt.grid()\n    plt.xlabel(\"n_components\")\n    plt.ylabel(\"% variance explained\")\n    plt.show()",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "univariate_mse_remove",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "univariate_mse_remove = ['selected', 'transfers_in_shift', 'transfers_out_shift', 'transfers_balance_shift']\nquasi_remove = ['penalties_saved_shift', 'penalties_missed_shift', 'own_goals_shift', 'red_cards_shift'] \nboruta_select= ['round' ,'value' ,'clean_sheets_shift','goals_scored_shift','saves_shift','assists_shift','bps_shift','influence_shift' ,'yellow_cards_shift' ,'creativity_shift','own_goals_shift'  ,       'red_cards_shift' ,        'minutes_shift' ,          'bonus_shift'   ,          'ict_index_shift' ,        'threat_shift' ,           'goals_conceded_shift',    'position_DEF' ,           'position_FWD',            'position_GK'  ,           'position_MID']        \nfwd_select = ['threat_shift', 'bonus_shift', 'creativity_shift', 'influence_shift', 'minutes_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'assists_shift', 'goals_scored_shift', 'yellow_cards_shift', 'bps_shift', 'penalties_saved_shift', 'saves_shift', 'red_cards_shift', 'own_goals_shift', 'position_MID', 'ict_index_shift', 'penalties_missed_shift', 'position_DEF', 'strength_defence_home', 'value', 'team_Man City', 'team_Newcastle', 'team_Arsenal', 'team_Chelsea', 'team_h_Sheffield Utd']\nbwd_select = ['value', 'strength_defence_home', 'clean_sheets_shift', 'goals_scored_shift', 'penalties_saved_shift', 'saves_shift', 'assists_shift', 'bps_shift', 'influence_shift', 'penalties_missed_shift', 'yellow_cards_shift', 'creativity_shift', 'own_goals_shift', 'red_cards_shift', 'minutes_shift', 'bonus_shift', 'ict_index_shift', 'threat_shift', 'goals_conceded_shift', 'position_DEF', 'position_FWD', 'position_GK', 'position_MID', 'team_Arsenal', 'team_Burnley', 'team_Man City', 'team_Newcastle', 'team_h_Other']\nvif_feat_lt_20 = ['team_a_difficulty', 'bps_shift', 'minutes_shift', 'goals_scored_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'bonus_shift', 'selected', 'value', 'saves_shift', 'assists_shift', 'team_h_score_shift', 'team_a_score_shift', 'yellow_cards_shift', 'penalties_saved_shift', 'round', 'penalties_missed_shift', 'red_cards_shift', 'own_goals_shift']\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# %%\n# %%\ndf_test, df_train, shift = read_data()",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "quasi_remove",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "quasi_remove = ['penalties_saved_shift', 'penalties_missed_shift', 'own_goals_shift', 'red_cards_shift'] \nboruta_select= ['round' ,'value' ,'clean_sheets_shift','goals_scored_shift','saves_shift','assists_shift','bps_shift','influence_shift' ,'yellow_cards_shift' ,'creativity_shift','own_goals_shift'  ,       'red_cards_shift' ,        'minutes_shift' ,          'bonus_shift'   ,          'ict_index_shift' ,        'threat_shift' ,           'goals_conceded_shift',    'position_DEF' ,           'position_FWD',            'position_GK'  ,           'position_MID']        \nfwd_select = ['threat_shift', 'bonus_shift', 'creativity_shift', 'influence_shift', 'minutes_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'assists_shift', 'goals_scored_shift', 'yellow_cards_shift', 'bps_shift', 'penalties_saved_shift', 'saves_shift', 'red_cards_shift', 'own_goals_shift', 'position_MID', 'ict_index_shift', 'penalties_missed_shift', 'position_DEF', 'strength_defence_home', 'value', 'team_Man City', 'team_Newcastle', 'team_Arsenal', 'team_Chelsea', 'team_h_Sheffield Utd']\nbwd_select = ['value', 'strength_defence_home', 'clean_sheets_shift', 'goals_scored_shift', 'penalties_saved_shift', 'saves_shift', 'assists_shift', 'bps_shift', 'influence_shift', 'penalties_missed_shift', 'yellow_cards_shift', 'creativity_shift', 'own_goals_shift', 'red_cards_shift', 'minutes_shift', 'bonus_shift', 'ict_index_shift', 'threat_shift', 'goals_conceded_shift', 'position_DEF', 'position_FWD', 'position_GK', 'position_MID', 'team_Arsenal', 'team_Burnley', 'team_Man City', 'team_Newcastle', 'team_h_Other']\nvif_feat_lt_20 = ['team_a_difficulty', 'bps_shift', 'minutes_shift', 'goals_scored_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'bonus_shift', 'selected', 'value', 'saves_shift', 'assists_shift', 'team_h_score_shift', 'team_a_score_shift', 'yellow_cards_shift', 'penalties_saved_shift', 'round', 'penalties_missed_shift', 'red_cards_shift', 'own_goals_shift']\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# %%\n# %%\ndf_test, df_train, shift = read_data()\ndf_test, df_train = change_different_teams(df_test, df_train)",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "fwd_select",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "fwd_select = ['threat_shift', 'bonus_shift', 'creativity_shift', 'influence_shift', 'minutes_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'assists_shift', 'goals_scored_shift', 'yellow_cards_shift', 'bps_shift', 'penalties_saved_shift', 'saves_shift', 'red_cards_shift', 'own_goals_shift', 'position_MID', 'ict_index_shift', 'penalties_missed_shift', 'position_DEF', 'strength_defence_home', 'value', 'team_Man City', 'team_Newcastle', 'team_Arsenal', 'team_Chelsea', 'team_h_Sheffield Utd']\nbwd_select = ['value', 'strength_defence_home', 'clean_sheets_shift', 'goals_scored_shift', 'penalties_saved_shift', 'saves_shift', 'assists_shift', 'bps_shift', 'influence_shift', 'penalties_missed_shift', 'yellow_cards_shift', 'creativity_shift', 'own_goals_shift', 'red_cards_shift', 'minutes_shift', 'bonus_shift', 'ict_index_shift', 'threat_shift', 'goals_conceded_shift', 'position_DEF', 'position_FWD', 'position_GK', 'position_MID', 'team_Arsenal', 'team_Burnley', 'team_Man City', 'team_Newcastle', 'team_h_Other']\nvif_feat_lt_20 = ['team_a_difficulty', 'bps_shift', 'minutes_shift', 'goals_scored_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'bonus_shift', 'selected', 'value', 'saves_shift', 'assists_shift', 'team_h_score_shift', 'team_a_score_shift', 'yellow_cards_shift', 'penalties_saved_shift', 'round', 'penalties_missed_shift', 'red_cards_shift', 'own_goals_shift']\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# %%\n# %%\ndf_test, df_train, shift = read_data()\ndf_test, df_train = change_different_teams(df_test, df_train)\nGW = 3\nmin_date = df_test[df_test['GW'] == GW]['kickoff_time'].min() # The first date of the gameweek",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "bwd_select",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "bwd_select = ['value', 'strength_defence_home', 'clean_sheets_shift', 'goals_scored_shift', 'penalties_saved_shift', 'saves_shift', 'assists_shift', 'bps_shift', 'influence_shift', 'penalties_missed_shift', 'yellow_cards_shift', 'creativity_shift', 'own_goals_shift', 'red_cards_shift', 'minutes_shift', 'bonus_shift', 'ict_index_shift', 'threat_shift', 'goals_conceded_shift', 'position_DEF', 'position_FWD', 'position_GK', 'position_MID', 'team_Arsenal', 'team_Burnley', 'team_Man City', 'team_Newcastle', 'team_h_Other']\nvif_feat_lt_20 = ['team_a_difficulty', 'bps_shift', 'minutes_shift', 'goals_scored_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'bonus_shift', 'selected', 'value', 'saves_shift', 'assists_shift', 'team_h_score_shift', 'team_a_score_shift', 'yellow_cards_shift', 'penalties_saved_shift', 'round', 'penalties_missed_shift', 'red_cards_shift', 'own_goals_shift']\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# %%\n# %%\ndf_test, df_train, shift = read_data()\ndf_test, df_train = change_different_teams(df_test, df_train)\nGW = 3\nmin_date = df_test[df_test['GW'] == GW]['kickoff_time'].min() # The first date of the gameweek\ndf_scl = df_train.append(df_test[df_test['GW'] <= GW]) # Only includes up to gameweek ",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "vif_feat_lt_20",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "vif_feat_lt_20 = ['team_a_difficulty', 'bps_shift', 'minutes_shift', 'goals_scored_shift', 'goals_conceded_shift', 'clean_sheets_shift', 'bonus_shift', 'selected', 'value', 'saves_shift', 'assists_shift', 'team_h_score_shift', 'team_a_score_shift', 'yellow_cards_shift', 'penalties_saved_shift', 'round', 'penalties_missed_shift', 'red_cards_shift', 'own_goals_shift']\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# %%\n# %%\ndf_test, df_train, shift = read_data()\ndf_test, df_train = change_different_teams(df_test, df_train)\nGW = 3\nmin_date = df_test[df_test['GW'] == GW]['kickoff_time'].min() # The first date of the gameweek\ndf_scl = df_train.append(df_test[df_test['GW'] <= GW]) # Only includes up to gameweek \nnum_feats = df_scl.select_dtypes(include='number').drop(columns = ['total_points_shift', 'GW']).columns",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "GW",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "GW = 3\nmin_date = df_test[df_test['GW'] == GW]['kickoff_time'].min() # The first date of the gameweek\ndf_scl = df_train.append(df_test[df_test['GW'] <= GW]) # Only includes up to gameweek \nnum_feats = df_scl.select_dtypes(include='number').drop(columns = ['total_points_shift', 'GW']).columns\nextracts = ['player_name', 'GW', 'total_points_shift', 'kickoff_time']\n# std_scaler_X, std_scaler_Y = StandardScaler(), StandardScaler()\n# df_scl[num_feats] = std_scaler_X.fit_transform(df_scl[num_feats])\n# %%\n# df_scl['total_points_shift'] = std_scaler_Y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n# %%",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "min_date",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "min_date = df_test[df_test['GW'] == GW]['kickoff_time'].min() # The first date of the gameweek\ndf_scl = df_train.append(df_test[df_test['GW'] <= GW]) # Only includes up to gameweek \nnum_feats = df_scl.select_dtypes(include='number').drop(columns = ['total_points_shift', 'GW']).columns\nextracts = ['player_name', 'GW', 'total_points_shift', 'kickoff_time']\n# std_scaler_X, std_scaler_Y = StandardScaler(), StandardScaler()\n# df_scl[num_feats] = std_scaler_X.fit_transform(df_scl[num_feats])\n# %%\n# df_scl['total_points_shift'] = std_scaler_Y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n# %%\ndf_scl = one_hot_encode(df_scl)",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "df_scl",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "df_scl = df_train.append(df_test[df_test['GW'] <= GW]) # Only includes up to gameweek \nnum_feats = df_scl.select_dtypes(include='number').drop(columns = ['total_points_shift', 'GW']).columns\nextracts = ['player_name', 'GW', 'total_points_shift', 'kickoff_time']\n# std_scaler_X, std_scaler_Y = StandardScaler(), StandardScaler()\n# df_scl[num_feats] = std_scaler_X.fit_transform(df_scl[num_feats])\n# %%\n# df_scl['total_points_shift'] = std_scaler_Y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n# %%\ndf_scl = one_hot_encode(df_scl)\ndf_train, df_test = df_scl[df_scl['kickoff_time'] < min_date], df_scl[df_scl['kickoff_time'] >= min_date] ",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "num_feats",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "num_feats = df_scl.select_dtypes(include='number').drop(columns = ['total_points_shift', 'GW']).columns\nextracts = ['player_name', 'GW', 'total_points_shift', 'kickoff_time']\n# std_scaler_X, std_scaler_Y = StandardScaler(), StandardScaler()\n# df_scl[num_feats] = std_scaler_X.fit_transform(df_scl[num_feats])\n# %%\n# df_scl['total_points_shift'] = std_scaler_Y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n# %%\ndf_scl = one_hot_encode(df_scl)\ndf_train, df_test = df_scl[df_scl['kickoff_time'] < min_date], df_scl[df_scl['kickoff_time'] >= min_date] \nX_train = df_train.drop(columns = extracts) ",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "extracts",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "extracts = ['player_name', 'GW', 'total_points_shift', 'kickoff_time']\n# std_scaler_X, std_scaler_Y = StandardScaler(), StandardScaler()\n# df_scl[num_feats] = std_scaler_X.fit_transform(df_scl[num_feats])\n# %%\n# df_scl['total_points_shift'] = std_scaler_Y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n# %%\ndf_scl = one_hot_encode(df_scl)\ndf_train, df_test = df_scl[df_scl['kickoff_time'] < min_date], df_scl[df_scl['kickoff_time'] >= min_date] \nX_train = df_train.drop(columns = extracts) \ny_train = df_train['total_points_shift']",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "df_scl",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "df_scl = one_hot_encode(df_scl)\ndf_train, df_test = df_scl[df_scl['kickoff_time'] < min_date], df_scl[df_scl['kickoff_time'] >= min_date] \nX_train = df_train.drop(columns = extracts) \ny_train = df_train['total_points_shift']\nX_test = df_test.drop(columns = extracts) \ny_test = df_test['total_points_shift']\n# %%\n# %%\n# test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW)\n# %%",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "X_train",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "X_train = df_train.drop(columns = extracts) \ny_train = df_train['total_points_shift']\nX_test = df_test.drop(columns = extracts) \ny_test = df_test['total_points_shift']\n# %%\n# %%\n# test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW)\n# %%\n# from numpy import mean\n# from numpy import std",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "y_train",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "y_train = df_train['total_points_shift']\nX_test = df_test.drop(columns = extracts) \ny_test = df_test['total_points_shift']\n# %%\n# %%\n# test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW)\n# %%\n# from numpy import mean\n# from numpy import std\n# from sklearn.datasets import make_regression",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "X_test",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "X_test = df_test.drop(columns = extracts) \ny_test = df_test['total_points_shift']\n# %%\n# %%\n# test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW)\n# %%\n# from numpy import mean\n# from numpy import std\n# from sklearn.datasets import make_regression\n# from sklearn.model_selection import cross_val_score",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "y_test",
        "kind": 5,
        "importPath": "src.3c_models",
        "description": "src.3c_models",
        "peekOfCode": "y_test = df_test['total_points_shift']\n# %%\n# %%\n# test_scaled_RF_model(X_train, y_train, X_test, y_test, std_scaler_Y, GW)\n# %%\n# from numpy import mean\n# from numpy import std\n# from sklearn.datasets import make_regression\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import RepeatedKFold",
        "detail": "src.3c_models",
        "documentation": {}
    },
    {
        "label": "get_models",
        "kind": 2,
        "importPath": "src.3d_stacked",
        "description": "src.3d_stacked",
        "peekOfCode": "def get_models():\n    models = list()\n    models.append(LinearRegression())\n    models.append(SVR())\n    models.append(KNeighborsRegressor())\n    models.append(RandomForestRegressor(n_estimators=10))\n    return models\n# cost function for base models\ndef rmse(yreal, yhat):\n    return sqrt(mean_squared_error(yreal, yhat))",
        "detail": "src.3d_stacked",
        "documentation": {}
    },
    {
        "label": "rmse",
        "kind": 2,
        "importPath": "src.3d_stacked",
        "description": "src.3d_stacked",
        "peekOfCode": "def rmse(yreal, yhat):\n    return sqrt(mean_squared_error(yreal, yhat))\n# create the super learner\ndef get_super_learner(X):\n    # preprocessing = {'pipeline-1': [StandardScaler(), Subset([0,1])], \n    preprocessing = {'pipeline-1': [Subset([5,6])], \n                     'pipeline-2': [Subset([5,6])]}\n    estimators = {'pipeline-1': [LinearRegression()], \n                  'pipeline-2': [LinearRegression()]}\n    ensemble = SuperLearner(scorer=rmse, folds=10, shuffle=True, sample_size=len(X))",
        "detail": "src.3d_stacked",
        "documentation": {}
    },
    {
        "label": "get_super_learner",
        "kind": 2,
        "importPath": "src.3d_stacked",
        "description": "src.3d_stacked",
        "peekOfCode": "def get_super_learner(X):\n    # preprocessing = {'pipeline-1': [StandardScaler(), Subset([0,1])], \n    preprocessing = {'pipeline-1': [Subset([5,6])], \n                     'pipeline-2': [Subset([5,6])]}\n    estimators = {'pipeline-1': [LinearRegression()], \n                  'pipeline-2': [LinearRegression()]}\n    ensemble = SuperLearner(scorer=rmse, folds=10, shuffle=True, sample_size=len(X))\n    ensemble.add(estimators, preprocessing)\n    ensemble.add_meta(LinearRegression()) \n    return ensemble",
        "detail": "src.3d_stacked",
        "documentation": {}
    },
    {
        "label": "ensemble",
        "kind": 5,
        "importPath": "src.3d_stacked",
        "description": "src.3d_stacked",
        "peekOfCode": "ensemble = get_super_learner(X)\n# fit the super learner\nensemble.fit(X, y)\n# %%\n# summarize base learners\nprint(ensemble.data)\n# %%\n# evaluate meta model\nyhat = ensemble.predict(X_val)\nprint('Super Learner: RMSE %.3f' % (rmse(y_val, yhat)))",
        "detail": "src.3d_stacked",
        "documentation": {}
    },
    {
        "label": "yhat",
        "kind": 5,
        "importPath": "src.3d_stacked",
        "description": "src.3d_stacked",
        "peekOfCode": "yhat = ensemble.predict(X_val)\nprint('Super Learner: RMSE %.3f' % (rmse(y_val, yhat)))\n# %%\nensemble.data\n# %%\nensemble.layers\n# %%",
        "detail": "src.3d_stacked",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def plot_results(true_value, predicted_value, title):\n    plt.figure(figsize=(10, 10))\n    plt.scatter(true_value, predicted_value, c='crimson')\n    p1 = max(max(predicted_value), max(true_value))\n    p2 = min(min(predicted_value), min(true_value))\n    plt.plot([p1, p2], [p1, p2], 'b-')\n    plt.xlabel('True Values', fontsize=15)\n    plt.ylabel('Predictions', fontsize=15)\n    plt.title(title)\n    plt.axis('equal')",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "regression_results",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def regression_results(y_true, y_pred, model_string):\n    print('# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>')\n    print(model_string)\n    explained_variance = metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error = metrics.mean_absolute_error(y_true, y_pred)\n    mse = metrics.mean_squared_error(y_true, y_pred)\n    r2 = metrics.r2_score(y_true, y_pred)\n    print('explained_variance: ', round(explained_variance, 4))\n    print('r2: ', round(r2, 4))\n    print('MAE: ', round(mean_absolute_error, 4))",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "baseline_model",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def baseline_model():\n    model = Sequential()\n    model.add(Dense(500, activation= \"relu\",))\n    model.add(Dense(500, activation= \"relu\"))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\ndef get_models():\n    models = list()\n    models.append(LinearRegression())",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "get_models",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def get_models():\n    models = list()\n    models.append(LinearRegression())\n    # models.append(SVR(C=100, epsilon=0.01, kernel='rbf'))\n    # models.append(KNeighborsRegressor(n_neighbors=3))\n    # models.append(RandomForestRegressor())\n    models.append(KerasRegressor(build_fn=baseline_model, epochs=50, verbose=False,validation_split=0.2))\n    return models\ndef get_out_of_fold_predictions(X, y, models):\n    meta_X, meta_y = list(), list()",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "get_out_of_fold_predictions",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def get_out_of_fold_predictions(X, y, models):\n    meta_X, meta_y = list(), list()\n    # define split of data\n    kfold = KFold(n_splits=5, shuffle=True)\n    # enumerate splits\n    for train_ix, test_ix in kfold.split(X):\n        fold_yhats = list()\n        # get data\n        train_X, test_X = X.iloc[train_ix, :], X.iloc[test_ix, :]\n        train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "fit_base_models",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def fit_base_models(X, y, models):\n    for model in models:\n        model.fit(X, y)\n    return models\ndef fit_meta_model(X, y):\n    model = KerasRegressor(build_fn=baseline_model, epochs=50,verbose=True, validation_split=0.2)\n    # model = LinearRegression()\n    model.fit(X, y)\n    return model\ndef evaluate_models(X, y, models, std_y):",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "fit_meta_model",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def fit_meta_model(X, y):\n    model = KerasRegressor(build_fn=baseline_model, epochs=50,verbose=True, validation_split=0.2)\n    # model = LinearRegression()\n    model.fit(X, y)\n    return model\ndef evaluate_models(X, y, models, std_y):\n    for model in models:\n        yhat = np.round(std_y.inverse_transform(model.predict(X)))\n        y = np.round(std_y.inverse_transform(y))\n        mse = mean_squared_error(y, yhat)",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "evaluate_models",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def evaluate_models(X, y, models, std_y):\n    for model in models:\n        yhat = np.round(std_y.inverse_transform(model.predict(X)))\n        y = np.round(std_y.inverse_transform(y))\n        mse = mean_squared_error(y, yhat)\n        print('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\ndef super_learner_predictions(X, models, meta_model):\n    meta_X = list()\n    for model in models:\n        yhat = model.predict(X)",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "super_learner_predictions",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def super_learner_predictions(X, models, meta_model):\n    meta_X = list()\n    for model in models:\n        yhat = model.predict(X)\n        meta_X.append(yhat.reshape(len(yhat),1))\n    meta_X = hstack(meta_X)\n    # predict\n    return meta_model.predict(meta_X)\ndef get_training_data(GW, num_feats):\n    df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv')",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "get_training_data",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def get_training_data(GW, num_feats):\n    df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//engineered_us.csv')\n    min_date = df[(df['GW'] == GW) & (df['kickoff_time'] >= '2020-08-12')]['kickoff_time'].min() # The first date of the gameweek\n    max_date = df[(df['GW'] == GW) & (df['kickoff_time'] > '2020-08-12')]['kickoff_time'].max() # The last date of the gameweek\n    df_scl = df[df['kickoff_time'] <= max_date].copy() # Only includes up to gameweek \n    std_x, std_y = StandardScaler(), StandardScaler()\n    df_scl[num_feats] = std_x.fit_transform(df_scl[num_feats])\n    df_scl['total_points_shift'] = std_y.fit_transform(df_scl['total_points_shift'].to_numpy().reshape(-1, 1))\n    df_train, df_test = df_scl[df_scl['kickoff_time'] < min_date], df_scl[df_scl['kickoff_time'] >= min_date] \n    X = df_train[num_feats] ",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "get_stacked_mod",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def get_stacked_mod(GW, num_feats):\n    X, X_val, y, y_val, std_x, std_y, df_test = get_training_data(GW, num_feats)\n    print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n    models = get_models()\n    meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n    print('Meta ', meta_X.shape, meta_y.shape)\n    models = fit_base_models(X, y, models)\n    meta_model = fit_meta_model(meta_X, meta_y)\n    evaluate_models(X_val, y_val, models, std_y)\n    yhat = super_learner_predictions(X_val, models, meta_model)",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "collect_results",
        "kind": 2,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "def collect_results():\n    results = []\n    for gw in range(1, 39):\n        print('Gameweek', gw)\n        results.append(get_stacked_mod(gw, num_feats))\n    df_orig = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_us.csv')\n    df_eng = pd.read_csv('C://Users//jd-vz//Desktop//Code//data/engineered_us.csv')\n    df = df_orig[['player_name', 'position', 'team', 'GW', 'kickoff_time']].copy()\n    df = df[df['kickoff_time'] > '2020-08-12']\n    df['total_points'] = df_eng['total_points_shift']",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "num_feats",
        "kind": 5,
        "importPath": "src.3e_stacked_manual",
        "description": "src.3e_stacked_manual",
        "peekOfCode": "num_feats = ['assists_shift',\n 'bonus_shift',\n 'form',\n 'goals_conceded_shift',\n 'bps_shift',\n 'total_points_shift_to_value',\n 'value',\n 'bps_shift_to_value',\n 'clean_sheets_shift',\n 'goals_scored_shift',",
        "detail": "src.3e_stacked_manual",
        "documentation": {}
    },
    {
        "label": "model_1",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "model_1 = pulp.LpProblem(\"Model1\",pulp.LpMaximize)\n#Other variables - generate the binary variables denoting the player position\nG = (GWValues['element_type']==1).astype(int).values\nD = (GWValues['element_type']==2).astype(int).values\nM = (GWValues['element_type']==3).astype(int).values\nF = (GWValues['element_type']==4).astype(int).values    \n# Define the parameter values for the team constraint\n# If player i from team j then t_i_j = 1\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "G = (GWValues['element_type']==1).astype(int).values\nD = (GWValues['element_type']==2).astype(int).values\nM = (GWValues['element_type']==3).astype(int).values\nF = (GWValues['element_type']==4).astype(int).values    \n# Define the parameter values for the team constraint\n# If player i from team j then t_i_j = 1\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(0,20):\n        if GWValues['team'].iloc[i] == j + 1:",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "D",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "D = (GWValues['element_type']==2).astype(int).values\nM = (GWValues['element_type']==3).astype(int).values\nF = (GWValues['element_type']==4).astype(int).values    \n# Define the parameter values for the team constraint\n# If player i from team j then t_i_j = 1\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(0,20):\n        if GWValues['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "M",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "M = (GWValues['element_type']==3).astype(int).values\nF = (GWValues['element_type']==4).astype(int).values    \n# Define the parameter values for the team constraint\n# If player i from team j then t_i_j = 1\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(0,20):\n        if GWValues['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "F",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "F = (GWValues['element_type']==4).astype(int).values    \n# Define the parameter values for the team constraint\n# If player i from team j then t_i_j = 1\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(0,20):\n        if GWValues['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Decision variables - eqn (4.3)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "t_i_j",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "t_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(0,20):\n        if GWValues['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Decision variables - eqn (4.3)\nx_vars_model_1  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_{}\".format(i))) for i in range(length_opt)}\n# Dummy variable for money left over after initial squad selected\nmoney_left_1 = pulp.LpVariable(cat=pulp.LpContinuous,lowBound=0, name = 'money_left_1')",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "t_i_j",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "t_i_j = np.array(t_i_j)\n# Decision variables - eqn (4.3)\nx_vars_model_1  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_{}\".format(i))) for i in range(length_opt)}\n# Dummy variable for money left over after initial squad selected\nmoney_left_1 = pulp.LpVariable(cat=pulp.LpContinuous,lowBound=0, name = 'money_left_1')\n# Objective Function - eqn (4.1) - Points here is a vector containing the points total for player i from the previous season\nmodel_1 += pulp.lpSum(Points[i]*x_vars_model_1[i] for i in range(length_opt)) , 'Objective Function'\n# Constraints\nmodel_1 += pulp.lpSum(costs[i]*x_vars_model_1[i] for i in range(length_opt)) + money_left_1 == 100, 'Cost constraint' # eqn (4.4)\nmodel_1 += pulp.lpSum(G[i]*x_vars_model_1[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint' # eqn (4.5)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "money_left_1",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "money_left_1 = pulp.LpVariable(cat=pulp.LpContinuous,lowBound=0, name = 'money_left_1')\n# Objective Function - eqn (4.1) - Points here is a vector containing the points total for player i from the previous season\nmodel_1 += pulp.lpSum(Points[i]*x_vars_model_1[i] for i in range(length_opt)) , 'Objective Function'\n# Constraints\nmodel_1 += pulp.lpSum(costs[i]*x_vars_model_1[i] for i in range(length_opt)) + money_left_1 == 100, 'Cost constraint' # eqn (4.4)\nmodel_1 += pulp.lpSum(G[i]*x_vars_model_1[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint' # eqn (4.5)\nmodel_1 += pulp.lpSum(D[i]*x_vars_model_1[i] for i in range(length_opt)) == 5, 'Defenders constraint' # eqn (4.6)\nmodel_1 += pulp.lpSum(M[i]*x_vars_model_1[i] for i in range(length_opt)) == 5, 'Midfielders constraint' # eqn (4.7)\nmodel_1 += pulp.lpSum(F[i]*x_vars_model_1[i] for i in range(length_opt)) == 3, 'Forwards constraint' # eqn (4.8)\nfor j in range(20):",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "xvars_res",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "xvars_res = []\nfor i in range(length_opt):\n    q = int(pulp.value(x_vars_model_1[i]))\n    xvars_res.append(q)\nxvars_res = np.array(xvars_res)\nplayers = X_values[xvars_res==1]\nm_gm1 = pulp.value(money_left_1) # Extract the amount of money left into a standard python variable\n# %% Second of three models\n# Updating the squad on a weekly basis\n#Other variables - defined as above",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "xvars_res",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "xvars_res = np.array(xvars_res)\nplayers = X_values[xvars_res==1]\nm_gm1 = pulp.value(money_left_1) # Extract the amount of money left into a standard python variable\n# %% Second of three models\n# Updating the squad on a weekly basis\n#Other variables - defined as above\nG = (gw_data['element_type']==1).astype(int).values\nD = (gw_data['element_type']==2).astype(int).values\nM = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    ",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "players",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "players = X_values[xvars_res==1]\nm_gm1 = pulp.value(money_left_1) # Extract the amount of money left into a standard python variable\n# %% Second of three models\n# Updating the squad on a weekly basis\n#Other variables - defined as above\nG = (gw_data['element_type']==1).astype(int).values\nD = (gw_data['element_type']==2).astype(int).values\nM = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "m_gm1",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "m_gm1 = pulp.value(money_left_1) # Extract the amount of money left into a standard python variable\n# %% Second of three models\n# Updating the squad on a weekly basis\n#Other variables - defined as above\nG = (gw_data['element_type']==1).astype(int).values\nD = (gw_data['element_type']==2).astype(int).values\nM = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "G = (gw_data['element_type']==1).astype(int).values\nD = (gw_data['element_type']==2).astype(int).values\nM = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "D",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "D = (gw_data['element_type']==2).astype(int).values\nM = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "M",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "M = (gw_data['element_type']==3).astype(int).values\nF = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Start defining the optimisation model",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "F",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "F = (gw_data['element_type']==4).astype(int).values    \nteams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Start defining the optimisation model\nmodel_2 = pulp.LpProblem('Model2',pulp.LpMaximize)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "teams",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "teams = gw_data['team']\nt_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Start defining the optimisation model\nmodel_2 = pulp.LpProblem('Model2',pulp.LpMaximize)\nm = 56 # An integer dummy variable used during the optimisationa - defined as M in (4.17) and (4.18)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "t_i_j",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "t_i_j = [[0 for y in range(20)] for x in range(length_opt)]\nfor i in range(length_opt):\n    for j in range(1,20):\n        if gw_data['team'].iloc[i] == j + 1:\n            t_i_j[i][j] = 1\nt_i_j = np.array(t_i_j)\n# Start defining the optimisation model\nmodel_2 = pulp.LpProblem('Model2',pulp.LpMaximize)\nm = 56 # An integer dummy variable used during the optimisationa - defined as M in (4.17) and (4.18)\noptimization_predictions = pred_method # Get the points prediction values",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "t_i_j",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "t_i_j = np.array(t_i_j)\n# Start defining the optimisation model\nmodel_2 = pulp.LpProblem('Model2',pulp.LpMaximize)\nm = 56 # An integer dummy variable used during the optimisationa - defined as M in (4.17) and (4.18)\noptimization_predictions = pred_method # Get the points prediction values\nallowed_aditional = 1 # The number of additional free transfers allowed\nm_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "model_2",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "model_2 = pulp.LpProblem('Model2',pulp.LpMaximize)\nm = 56 # An integer dummy variable used during the optimisationa - defined as M in (4.17) and (4.18)\noptimization_predictions = pred_method # Get the points prediction values\nallowed_aditional = 1 # The number of additional free transfers allowed\nm_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "m = 56 # An integer dummy variable used during the optimisationa - defined as M in (4.17) and (4.18)\noptimization_predictions = pred_method # Get the points prediction values\nallowed_aditional = 1 # The number of additional free transfers allowed\nm_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "optimization_predictions",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "optimization_predictions = pred_method # Get the points prediction values\nallowed_aditional = 1 # The number of additional free transfers allowed\nm_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "allowed_aditional",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "allowed_aditional = 1 # The number of additional free transfers allowed\nm_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "m_gm1",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "m_gm1 = float(np.array(report.iloc[(GW-2):(GW-1), 2:3])) \nxvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "xvars_res",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "xvars_res = x_final_chosen\n# Decision variables\nx_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "x_g",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "x_g = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"x_g_{}\".format(i))) for i in range(length_opt)} # x_i,g\nmoney_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "money_left_2",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "money_left_2 = pulp.LpVariable(cat = pulp.LpContinuous, lowBound = 0, name = 'money left model 2') # The amount of money left after the week\n# Additional decision variables\nt_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'\nmodel_2 += pulp.lpSum(D[i]*x_g[i] for i in range(length_opt)) == 5, 'Defenders constraint'",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "t_g",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "t_g = pulp.LpVariable(cat = pulp.LpInteger, name = 't_g', lowBound = 0) #The number of transfers performed\nf_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'\nmodel_2 += pulp.lpSum(D[i]*x_g[i] for i in range(length_opt)) == 5, 'Defenders constraint'\nmodel_2 += pulp.lpSum(M[i]*x_g[i] for i in range(length_opt)) == 5, 'Midfielders constraint'\nmodel_2 += pulp.lpSum(F[i]*x_g[i] for i in range(length_opt)) == 3, 'Forwards constraint'    ",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "f_g",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "f_g = pulp.LpVariable(cat = pulp.LpInteger, name = 'f_g', lowBound = 0) #The number of free transfers available\ny_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'\nmodel_2 += pulp.lpSum(D[i]*x_g[i] for i in range(length_opt)) == 5, 'Defenders constraint'\nmodel_2 += pulp.lpSum(M[i]*x_g[i] for i in range(length_opt)) == 5, 'Midfielders constraint'\nmodel_2 += pulp.lpSum(F[i]*x_g[i] for i in range(length_opt)) == 3, 'Forwards constraint'    \nmodel_2 +=  f_g <= 1, 'Free trades constraint' ",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "y_model_2",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "y_model_2 = pulp.LpVariable(cat = pulp.LpBinary, name = 'y_model_2') # Dummy variable for the if-else constraints (4.17) and (4.18)\nq_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'\nmodel_2 += pulp.lpSum(D[i]*x_g[i] for i in range(length_opt)) == 5, 'Defenders constraint'\nmodel_2 += pulp.lpSum(M[i]*x_g[i] for i in range(length_opt)) == 5, 'Midfielders constraint'\nmodel_2 += pulp.lpSum(F[i]*x_g[i] for i in range(length_opt)) == 3, 'Forwards constraint'    \nmodel_2 +=  f_g <= 1, 'Free trades constraint' \nmodel_2 += -q_g +4*(t_g-f_g) <= m*y_model_2, 'if' # eqn (4.17)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "q_g",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "q_g = pulp.LpVariable(cat=pulp.LpContinuous, lowBound = 0, name = 'q_g') # Variable for the points penalty incurred due to more transfers than the number of free transfers being performed\n# Constraints\nmodel_2 += m_gm1 + pulp.lpSum(costs[i]*xvars_res[i] for i in range(length_opt)) == money_left_2 + pulp.lpSum(costs[i]*x_g[i] for i in range(length_opt)), 'Bookkeeping constraint' # Ensure budget is not exceeded - eqn (4.14)\nmodel_2 += pulp.lpSum(G[i]*x_g[i] for i in range(length_opt)) == 2, 'Goalkeepers constraint'\nmodel_2 += pulp.lpSum(D[i]*x_g[i] for i in range(length_opt)) == 5, 'Defenders constraint'\nmodel_2 += pulp.lpSum(M[i]*x_g[i] for i in range(length_opt)) == 5, 'Midfielders constraint'\nmodel_2 += pulp.lpSum(F[i]*x_g[i] for i in range(length_opt)) == 3, 'Forwards constraint'    \nmodel_2 +=  f_g <= 1, 'Free trades constraint' \nmodel_2 += -q_g +4*(t_g-f_g) <= m*y_model_2, 'if' # eqn (4.17)\nmodel_2 += t_g - f_g <= m*(1-y_model_2), 'else'  # eqn (4.18)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "trades",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "trades = pulp.value(t_g)\nm_gm2 = pulp.value(money_left_2)\nmoney_prev = m_gm2\npenalty = pulp.value(q_g)\n# %% Final model\n# Model for selecting a starting 11\n# Let's determine the starting eleven\nmodel_3 = pulp.LpProblem('Model3', pulp.LpMaximize)\noptimization_predictions = pred_method # et the points prediction values\n# Decision variables",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "m_gm2",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "m_gm2 = pulp.value(money_left_2)\nmoney_prev = m_gm2\npenalty = pulp.value(q_g)\n# %% Final model\n# Model for selecting a starting 11\n# Let's determine the starting eleven\nmodel_3 = pulp.LpProblem('Model3', pulp.LpMaximize)\noptimization_predictions = pred_method # et the points prediction values\n# Decision variables\ns_model_3  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"s_{}\".format(i))) for i in range(length_opt)} # eqn (4.25)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "money_prev",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "money_prev = m_gm2\npenalty = pulp.value(q_g)\n# %% Final model\n# Model for selecting a starting 11\n# Let's determine the starting eleven\nmodel_3 = pulp.LpProblem('Model3', pulp.LpMaximize)\noptimization_predictions = pred_method # et the points prediction values\n# Decision variables\ns_model_3  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"s_{}\".format(i))) for i in range(length_opt)} # eqn (4.25)\n# Constraints",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "penalty",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "penalty = pulp.value(q_g)\n# %% Final model\n# Model for selecting a starting 11\n# Let's determine the starting eleven\nmodel_3 = pulp.LpProblem('Model3', pulp.LpMaximize)\noptimization_predictions = pred_method # et the points prediction values\n# Decision variables\ns_model_3  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"s_{}\".format(i))) for i in range(length_opt)} # eqn (4.25)\n# Constraints\nmodel_3 += pulp.lpSum(s_model_3[i] for i in range(length_opt)) == 11, '11 constraint' # eqn (4.26)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "model_3",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "model_3 = pulp.LpProblem('Model3', pulp.LpMaximize)\noptimization_predictions = pred_method # et the points prediction values\n# Decision variables\ns_model_3  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"s_{}\".format(i))) for i in range(length_opt)} # eqn (4.25)\n# Constraints\nmodel_3 += pulp.lpSum(s_model_3[i] for i in range(length_opt)) == 11, '11 constraint' # eqn (4.26)\nmodel_3 += pulp.lpSum(G[i]*s_model_3[i] for i in range(length_opt)) == 1, 'GK constraint' # eqn (4.27)\nmodel_3 += pulp.lpSum(D[i]*s_model_3[i] for i in range(length_opt)) >= 3, 'Defenders constraint' # eqn (4.28)\nmodel_3 += pulp.lpSum(F[i]*s_model_3[i] for i in range(length_opt)) >= 1, 'Forwards constraint' # eqn (4.29)\n# Objective function",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "optimization_predictions",
        "kind": 5,
        "importPath": "src.4a_lp",
        "description": "src.4a_lp",
        "peekOfCode": "optimization_predictions = pred_method # et the points prediction values\n# Decision variables\ns_model_3  = {i:pulp.LpVariable(cat=pulp.LpBinary, name=(\"s_{}\".format(i))) for i in range(length_opt)} # eqn (4.25)\n# Constraints\nmodel_3 += pulp.lpSum(s_model_3[i] for i in range(length_opt)) == 11, '11 constraint' # eqn (4.26)\nmodel_3 += pulp.lpSum(G[i]*s_model_3[i] for i in range(length_opt)) == 1, 'GK constraint' # eqn (4.27)\nmodel_3 += pulp.lpSum(D[i]*s_model_3[i] for i in range(length_opt)) >= 3, 'Defenders constraint' # eqn (4.28)\nmodel_3 += pulp.lpSum(F[i]*s_model_3[i] for i in range(length_opt)) >= 1, 'Forwards constraint' # eqn (4.29)\n# Objective function\nmodel_3 += pulp.lpSum(optimization_predictions[i]*x_final_chosen[i]*s_model_3[i] for i in range(length_opt)), 'Objective function' # eqn (4.24)",
        "detail": "src.4a_lp",
        "documentation": {}
    },
    {
        "label": "scale_numeric",
        "kind": 2,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "def scale_numeric(df):\n    std_scaler_X = StandardScaler()\n    std_scaler_Y = StandardScaler()\n    df_scaled = pd.DataFrame(std_scaler_X.fit_transform(df.drop(columns = ['total_points'], axis = 1).values), columns=df.drop(columns = ['total_points'], axis = 1).columns, index=df.index)\n    df_scaled['total_points'] = std_scaler_Y.fit_transform(df['total_points'].to_numpy().reshape(-1, 1))\n    return df_scaled, std_scaler_X, std_scaler_Y\ndef test_baseline(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)\n    print('Baseline contains: ', X_train.shape[0], 'entries')# summarize the shape of the train and test sets\n    model = LinearRegression()",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "test_baseline",
        "kind": 2,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "def test_baseline(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)\n    print('Baseline contains: ', X_train.shape[0], 'entries')# summarize the shape of the train and test sets\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    yhat = model.predict(X_test)\n    mae = mean_absolute_error(y_test, yhat)\n    print('Baseline: MAE: %.3f' % mae)\n    return X_train, y_train, X_test, y_test\ndef test_results(model, model_str, X_train, X_test, y_train, y_test, parameter = None):",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "test_results",
        "kind": 2,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "def test_results(model, model_str, X_train, X_test, y_train, y_test, parameter = None):\n    yhat = model.fit_predict(X_train)\n    mask = yhat != -1\n    model = LinearRegression()\n    model.fit(X_train.loc[mask, :], y_train.loc[mask])\n    print(f'{model_str}: removed ', X_train.shape[0] - X_train.loc[mask, :].shape[0], ' entries' )\n    if parameter is not None:\n        print('With ', parameter)\n    yhat = model.predict(X_test)\n    mae = mean_absolute_error(y_test, yhat)",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "check_data_dist",
        "kind": 2,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "def check_data_dist(df, feat = None):\n    if feat is None:\n        for feat in df.select_dtypes('number').columns:\n            dist_fit = Fitter(df[feat], timeout=60*10, distributions=get_common_distributions())\n            dist_fit.fit()\n            key, value = list(dist_fit.get_best(method = 'sumsquare_error').items())[0] # Key is the identified distribution\n            print(f'{feat} has {key} distribution')\n    else:\n            dist_fit = Fitter(df[feat], timeout=60*10, distributions=get_common_distributions())\n            dist_fit.fit()",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "df = read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//cleaned_fpl.csv', index_col=0)\nX, y = df.select_dtypes(include = 'number').drop(['total_points'], axis=1),df['total_points'] # split into input and output elements\nX_train, y_train, X_test, y_test = test_baseline(X, y)\ncheck_data_dist(df)\n# %%\ndist_fit = Fitter(df['total_points'], timeout=60*10, distributions=['expon', 'norm'])\ndist_fit.fit()\ndist_fit.summary()\n# %%",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "dist_fit",
        "kind": 5,
        "importPath": "src.check_outliers",
        "description": "src.check_outliers",
        "peekOfCode": "dist_fit = Fitter(df['total_points'], timeout=60*10, distributions=['expon', 'norm'])\ndist_fit.fit()\ndist_fit.summary()\n# %%\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>",
        "detail": "src.check_outliers",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "src.construct_data",
        "description": "src.construct_data",
        "peekOfCode": "def read_data(season = '2019-00'):\n    \"\"\"[This function reads the data and defines the features that will be averaged]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    understat = delete_any_duplicates(pd.read_csv(f'C://Users//jd-vz//Desktop//Code//data//{season}//training//cleaned_understat.csv', index_col=0))\n    fpl = delete_any_duplicates(pd.read_csv(f'C://Users//jd-vz//Desktop//Code//data//{season}//training//cleaned_fpl.csv', index_col=0))\n    fpl_avg = ['assists', 'bonus', 'bps', 'clean_sheets', 'creativity', 'goals_conceded', 'goals_scored', \n               'ict_index', 'influence', 'saves', 'threat']\n    understat_avg = ['assists', 'bonus', 'bps', 'clean_sheets', 'creativity', 'goals_conceded', 'goals_scored', ",
        "detail": "src.construct_data",
        "documentation": {}
    },
    {
        "label": "rolling_avg",
        "kind": 2,
        "importPath": "src.construct_data",
        "description": "src.construct_data",
        "peekOfCode": "def rolling_avg(data, prev_games, feats):\n    \"\"\"[This function creates a previous game rolling average for the selected features]\n    Args:\n        data ([type]): [description]\n        prev_games ([type]): [description]\n        feats ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    new_feats = []",
        "detail": "src.construct_data",
        "documentation": {}
    },
    {
        "label": "plot_player",
        "kind": 2,
        "importPath": "src.construct_data",
        "description": "src.construct_data",
        "peekOfCode": "def plot_player(player, feature, prev_games, data):\n    subset = data.loc[data.player_name == player]\n    plt.plot(subset['GW'], subset[feature], label = feature)\n    # plt.plot(subset['GW'], subset[feature + '_last_' + str(prev_games)], label = feature +  '_last_' + str(prev_games))\n    plt.legend()\n    plt.show()\ndef plot_Ozil(understat_avg, prev_games, understat):\n    for feat in understat_avg:\n        plot_player('Mesut Özil', feat, prev_games=prev_games, data = understat)\ndef main(season, prev_games):",
        "detail": "src.construct_data",
        "documentation": {}
    },
    {
        "label": "plot_Ozil",
        "kind": 2,
        "importPath": "src.construct_data",
        "description": "src.construct_data",
        "peekOfCode": "def plot_Ozil(understat_avg, prev_games, understat):\n    for feat in understat_avg:\n        plot_player('Mesut Özil', feat, prev_games=prev_games, data = understat)\ndef main(season, prev_games):\n    fpl, fpl_avg, understat, understat_avg = read_data(season)\n    fpl = fpl.groupby(['player_name']).apply(rolling_avg, prev_games = prev_games, feats=fpl_avg)\n    understat = understat.groupby(['player_name']).apply(rolling_avg, prev_games = prev_games, feats=understat_avg)\n    # plot_Ozil(understat_avg, prev_games, understat)\nmain(season='2019-20', prev_games = 1)\n# %%",
        "detail": "src.construct_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.construct_data",
        "description": "src.construct_data",
        "peekOfCode": "def main(season, prev_games):\n    fpl, fpl_avg, understat, understat_avg = read_data(season)\n    fpl = fpl.groupby(['player_name']).apply(rolling_avg, prev_games = prev_games, feats=fpl_avg)\n    understat = understat.groupby(['player_name']).apply(rolling_avg, prev_games = prev_games, feats=understat_avg)\n    # plot_Ozil(understat_avg, prev_games, understat)\nmain(season='2019-20', prev_games = 1)\n# %%\nif __name__ == '__main__':\n    fpl, fpl_avg, understat, understat_avg = read_data(season='2019-20')\n    plot_Ozil(understat_avg=fpl.columns.drop(['GW', 'player_name','home_True', 'position_DEF', 'position_FWD',",
        "detail": "src.construct_data",
        "documentation": {}
    },
    {
        "label": "time_series_plot",
        "kind": 2,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "def time_series_plot(df):\n    \"\"\"Given dataframe, generate times series plot of numeric data by daily, monthly and yearly frequency\"\"\"\n    print(\"\\nTo check time series of numeric data  by daily, monthly and yearly frequency\")\n    if len(df.select_dtypes(include='datetime64').columns)>0:\n        for col in df.select_dtypes(include='datetime64').columns:\n            for p in ['D', 'M', 'Y']:\n                if p=='D':\n                    print(\"Plotting daily data\")\n                elif p=='M':\n                    print(\"Plotting monthly data\")",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "numeric_eda",
        "kind": 2,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "def numeric_eda(df, hue=None):\n    \"\"\"Given dataframe, generate EDA of numeric data\"\"\"\n    print(\"\\nTo check: \\nDistribution of numeric data\")\n    display(df.describe().T)\n    columns = df.select_dtypes(include=np.number).columns\n    figure = plt.figure(figsize=(20, 10))\n    figure.add_subplot(1, len(columns), 1)\n    for index, col in enumerate(columns):\n        if index > 0:\n            figure.add_subplot(1, len(columns), index + 1)",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "top5",
        "kind": 2,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "def top5(df):\n    \"\"\"Given dataframe, generate top 5 unique values for non-numeric data\"\"\"\n    columns = df.select_dtypes(include=['object', 'category']).columns\n    for col in columns:\n        print(\"Top 5 unique values of \" + col)\n        print(df[col].value_counts().reset_index().rename(columns={\"index\": col, col: \"Count\"})[\n              :min(5, len(df[col].value_counts()))])\n        print(\" \")\ndef categorical_eda(df, hue=None):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "categorical_eda",
        "kind": 2,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "def categorical_eda(df, hue=None):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"\n    print(\"\\nTo check: \\nUnique count of non-numeric data\\n\")\n    print(df.select_dtypes(include=['object', 'category']).nunique())\n    top5(df)\n    # Plot count distribution of categorical data\n    for col in df.select_dtypes(include='category').columns:\n        fig = sns.catplot(x=col, kind=\"count\", data=df, hue=hue)\n        fig.set_xticklabels(rotation=90)\n        plt.show()",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "eda",
        "kind": 2,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "def eda(df):\n    \"\"\"Given dataframe, generate exploratory data analysis\"\"\"\n    # check that input is pandas dataframe\n    if type(df) != pd.core.frame.DataFrame:\n        raise TypeError(\"Only pandas dataframe is allowed as input\")\n    # replace field that's entirely space (or empty) with NaN\n    df = df.replace(r'^\\s*$', np.nan, regex=True)\n    print(\"Preview of data:\")\n    display(df.head(3))\n    print(\"\\nTo check: \\n (1) Total number of entries \\n (2) Column types \\n (3) Any null values\\n\")",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//cleaned_fpl.csv', index_col=0)\ndef time_series_plot(df):\n    \"\"\"Given dataframe, generate times series plot of numeric data by daily, monthly and yearly frequency\"\"\"\n    print(\"\\nTo check time series of numeric data  by daily, monthly and yearly frequency\")\n    if len(df.select_dtypes(include='datetime64').columns)>0:\n        for col in df.select_dtypes(include='datetime64').columns:\n            for p in ['D', 'M', 'Y']:\n                if p=='D':\n                    print(\"Plotting daily data\")\n                elif p=='M':",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "df['kickoff_time']",
        "kind": 5,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "df['kickoff_time'] = pd.to_datetime(df['kickoff_time'])\ntime_series_plot(df)\n# %%\n# set datetime data\nimport seaborn as sns\nsns.violinplot(x = 'position', y = 'total_points', data = df)\n# %%\ndf.groupby('Sex').size",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "sns.violinplot(x",
        "kind": 5,
        "importPath": "src.explore_data",
        "description": "src.explore_data",
        "peekOfCode": "sns.violinplot(x = 'position', y = 'total_points', data = df)\n# %%\ndf.groupby('Sex').size",
        "detail": "src.explore_data",
        "documentation": {}
    },
    {
        "label": "check_shift",
        "kind": 2,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "def check_shift():\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//2019-20//training//'\n    df = pd.read_csv(training_path + 'cleaned_fpl.csv')\n    df_shift = pd.read_csv(training_path + 'shifted_fpl.csv')\n    a, b = df[df.player_name == 'Aaron Cresswell'][['GW', 'total_points', 'bps']], df_shift[df_shift.player_name == 'Aaron Cresswell'][['GW', 'total_points_shift', 'bps_shift']]\n    print(pd.merge(a, b, on='GW'))\ndef check_dups():\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//2019-20//training//'\n    df = pd.read_csv(training_path + 'cleaned_fpl.csv')\n    df_shift = pd.read_csv(training_path + 'shifted_fpl.csv')",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "check_dups",
        "kind": 2,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "def check_dups():\n    training_path = f'C://Users//jd-vz//Desktop//Code//data//2019-20//training//'\n    df = pd.read_csv(training_path + 'cleaned_fpl.csv')\n    df_shift = pd.read_csv(training_path + 'shifted_fpl.csv')\n    print(df[df.duplicated(['player_name', 'GW'])])\n# %%\nimport pandas as pd\ntraining_path = 'C://Users//jd-vz//Desktop//Code//data//2020-21//'\ndf = pd.read_csv(training_path + 'players_raw.csv')\nprint(df[['first_name', 'second_name', 'id','team', 'element_type', 'goals_scored', 'total_points']].iloc[300:305:,].to_latex())",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "count_fixtures",
        "kind": 2,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "def count_fixtures():\n    df1 = pd.read_csv(training_path + 'collected_us.csv')\n    import numpy as np\n    df1['cnt'] = np.where(df1['was_home'], df1['kickoff_time'] + df1['team'] + df1['opponent_team'],  df1['kickoff_time'] + df1['opponent_team']  + df1['team'] )\n    print(df1['cnt'].nunique())\ncount_fixtures()\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_fpl.csv')\nprint(df[df['player_name'] == 'Yves Bissouma'][['player_name', 'team', 'position', 'GW', 'season', 'total_points']].tail(6).to_latex())\n# %%",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "training_path",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "training_path = 'C://Users//jd-vz//Desktop//Code//data//2020-21//'\ndf = pd.read_csv(training_path + 'players_raw.csv')\nprint(df[['first_name', 'second_name', 'id','team', 'element_type', 'goals_scored', 'total_points']].iloc[300:305:,].to_latex())\n# %%\ntraining_path = 'C://Users//jd-vz//Desktop//Code//data//2020-21//players//Aaron_Connolly_78//'\ndf = pd.read_csv(training_path + 'gw.csv')\n# %%\nprint(df[['element', 'fixture', 'kickoff_time', 'value', 'selected', 'total_points']].head().to_latex())\n# %%\n# %%",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.read_csv(training_path + 'players_raw.csv')\nprint(df[['first_name', 'second_name', 'id','team', 'element_type', 'goals_scored', 'total_points']].iloc[300:305:,].to_latex())\n# %%\ntraining_path = 'C://Users//jd-vz//Desktop//Code//data//2020-21//players//Aaron_Connolly_78//'\ndf = pd.read_csv(training_path + 'gw.csv')\n# %%\nprint(df[['element', 'fixture', 'kickoff_time', 'value', 'selected', 'total_points']].head().to_latex())\n# %%\n# %%\nimport pandas as pd",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "training_path",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "training_path = 'C://Users//jd-vz//Desktop//Code//data//2020-21//players//Aaron_Connolly_78//'\ndf = pd.read_csv(training_path + 'gw.csv')\n# %%\nprint(df[['element', 'fixture', 'kickoff_time', 'value', 'selected', 'total_points']].head().to_latex())\n# %%\n# %%\nimport pandas as pd\ntraining_path = 'C://Users//jd-vz//Desktop//Code//data//'\ndef count_fixtures():\n    df1 = pd.read_csv(training_path + 'collected_us.csv')",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.read_csv(training_path + 'gw.csv')\n# %%\nprint(df[['element', 'fixture', 'kickoff_time', 'value', 'selected', 'total_points']].head().to_latex())\n# %%\n# %%\nimport pandas as pd\ntraining_path = 'C://Users//jd-vz//Desktop//Code//data//'\ndef count_fixtures():\n    df1 = pd.read_csv(training_path + 'collected_us.csv')\n    import numpy as np",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "training_path",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "training_path = 'C://Users//jd-vz//Desktop//Code//data//'\ndef count_fixtures():\n    df1 = pd.read_csv(training_path + 'collected_us.csv')\n    import numpy as np\n    df1['cnt'] = np.where(df1['was_home'], df1['kickoff_time'] + df1['team'] + df1['opponent_team'],  df1['kickoff_time'] + df1['opponent_team']  + df1['team'] )\n    print(df1['cnt'].nunique())\ncount_fixtures()\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_fpl.csv')\nprint(df[df['player_name'] == 'Yves Bissouma'][['player_name', 'team', 'position', 'GW', 'season', 'total_points']].tail(6).to_latex())",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//collected_fpl.csv')\nprint(df[df['player_name'] == 'Yves Bissouma'][['player_name', 'team', 'position', 'GW', 'season', 'total_points']].tail(6).to_latex())\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')\n# %%\nprint(df.loc[[31,485, 419],['player_name', 'date', 'goals', 'xG', 'xG', 'npg', 'npxG' ]].to_latex())\n# %%\nname_mapper = {'Adrián':'Adrián Bernabé', # Contains both seasons corrections\n                   'Alisson':'Alisson Ramses Becker',\n                   'Allan':'Allan Marques Loureiro',",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')\n# %%\nprint(df.loc[[31,485, 419],['player_name', 'date', 'goals', 'xG', 'xG', 'npg', 'npxG' ]].to_latex())\n# %%\nname_mapper = {'Adrián':'Adrián Bernabé', # Contains both seasons corrections\n                   'Alisson':'Alisson Ramses Becker',\n                   'Allan':'Allan Marques Loureiro',\n                   'André Gomes':'André Filipe Tavares Gomes',\n                   'Angelino':'José Ángel Esmorís Tasende',\n                   'Bernard':'Bernard Anício Caldeira Duarte', # Everton",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "name_mapper",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "name_mapper = {'Adrián':'Adrián Bernabé', # Contains both seasons corrections\n                   'Alisson':'Alisson Ramses Becker',\n                   'Allan':'Allan Marques Loureiro',\n                   'André Gomes':'André Filipe Tavares Gomes',\n                   'Angelino':'José Ángel Esmorís Tasende',\n                   'Bernard':'Bernard Anício Caldeira Duarte', # Everton\n                   'Bernardo Silva':'Bernardo Mota Veiga de Carvalho e Silva', # Manchester City\n                   'Bernardo':'Bernardo Fernandes da Silva Junior', # \n                   'Borja Bastón':'Borja González Tomás',\n                   'Chicharito':'Javier Hernández Balcázar',",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "new_name",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "new_name = [elem for elem in name_mapper.values()]\nold_name = [elem for elem in name_mapper.keys()]\ndf = pd.DataFrame()\ndf['Understat'] = old_name\ndf['FPL'] = new_name\n# %%\nprint(df.to_latex())\n# %%\n# %%\nprint(pd.DataFrame(name_mapper).to_latex())",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "old_name",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "old_name = [elem for elem in name_mapper.keys()]\ndf = pd.DataFrame()\ndf['Understat'] = old_name\ndf['FPL'] = new_name\n# %%\nprint(df.to_latex())\n# %%\n# %%\nprint(pd.DataFrame(name_mapper).to_latex())\n# %%",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.DataFrame()\ndf['Understat'] = old_name\ndf['FPL'] = new_name\n# %%\nprint(df.to_latex())\n# %%\n# %%\nprint(pd.DataFrame(name_mapper).to_latex())\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df['Understat']",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df['Understat'] = old_name\ndf['FPL'] = new_name\n# %%\nprint(df.to_latex())\n# %%\n# %%\nprint(pd.DataFrame(name_mapper).to_latex())\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')\n# %%",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df['FPL']",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df['FPL'] = new_name\n# %%\nprint(df.to_latex())\n# %%\n# %%\nprint(pd.DataFrame(name_mapper).to_latex())\n# %%\ndf = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')\n# %%\nlist = ['Tottenham', 'Aston Villa', 'Wolverhampton Wanderers',",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2020-21//understat//all_understat_players.csv')\n# %%\nlist = ['Tottenham', 'Aston Villa', 'Wolverhampton Wanderers',\n       'Sheffield United', 'Manchester United', 'Crystal Palace',\n       'Burnley', 'West Bromwich Albion', 'Liverpool', 'Fulham', 'Leeds',\n       'Leicester', 'Arsenal', 'Manchester City', 'Brighton', 'West Ham',\n       'Newcastle United', 'Everton', 'Southampton', 'Chelsea', 'Reims',\n       'Rennes', 'Monaco', 'Montpellier', 'Lorient', 'Bordeaux',\n       'Saint-Etienne', 'Brest', 'Nimes', 'Dijon', 'Nice', 'Lyon',\n       'Nantes', 'Real Sociedad', 'Osasuna', 'Atletico Madrid',",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "list",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "list = ['Tottenham', 'Aston Villa', 'Wolverhampton Wanderers',\n       'Sheffield United', 'Manchester United', 'Crystal Palace',\n       'Burnley', 'West Bromwich Albion', 'Liverpool', 'Fulham', 'Leeds',\n       'Leicester', 'Arsenal', 'Manchester City', 'Brighton', 'West Ham',\n       'Newcastle United', 'Everton', 'Southampton', 'Chelsea', 'Reims',\n       'Rennes', 'Monaco', 'Montpellier', 'Lorient', 'Bordeaux',\n       'Saint-Etienne', 'Brest', 'Nimes', 'Dijon', 'Nice', 'Lyon',\n       'Nantes', 'Real Sociedad', 'Osasuna', 'Atletico Madrid',\n       'Villarreal', 'Granada', 'SD Huesca', 'Valencia', 'Celta Vigo',\n       'Alaves', 'Real Madrid', 'Eintracht Frankfurt', 'Mainz 05',",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "tms",
        "kind": 5,
        "importPath": "src.latex_utilities",
        "description": "src.latex_utilities",
        "peekOfCode": "tms = ['Paris Saint Germain', 'Lens', 'Lille', 'Strasbourg', 'Metz', 'Marseille', 'Verona', 'Fiorentina']\nprint(df.loc[df['h_team'].isin(tms)][['player_name', 'date', 'xG', 'h_team', 'a_team']].head(30).to_latex())\n# %%",
        "detail": "src.latex_utilities",
        "documentation": {}
    },
    {
        "label": "read_data",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def read_data(season):\n    \"\"\"[This function reads the data]\n    Args:\n        season ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\n    fpl = pd.read_csv(training_path + 'cleaned_fpl.csv', index_col=0)\n    understat = pd.read_csv(training_path + 'cleaned_understat.csv', index_col=0)",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "quasi_constant_fs",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def quasi_constant_fs(data,threshold=0.99):\n    \"\"\" detect features that show the same value for the \n    majority/all of the observations (constant/quasi-constant features)\n    Parameters\n    ----------\n    data : pd.Dataframe\n    threshold : threshold to identify the variable as constant\n    Returns\n    -------\n    list of variables names",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "corr_feature_detect",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def corr_feature_detect(data,threshold=0.8):\n    \"\"\" detect highly-correlated features of a Dataframe\n    Parameters\n    ----------\n    data : pd.Dataframe\n    threshold : threshold to identify the variable correlated\n    Returns\n    -------\n    pairs of correlated variables\n    \"\"\"",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "mutual_info",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def mutual_info(df,select_k=10):\n    df = df.select_dtypes(include='number') # NB: Only includes numeric\n    X = df.drop(columns = ['total_points'])\n    y = df['total_points']\n    if select_k >= 1:\n        sel_ = SelectKBest(mutual_info_classif, k=select_k).fit(X,y)\n        col = X.columns[sel_.get_support()]\n    elif 0 < select_k < 1:\n        sel_ = SelectPercentile(mutual_info_classif, percentile=select_k*100).fit(X,y)\n        col = X.columns[sel_.get_support()]   ",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "univariate_mse",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def univariate_mse(df,threshold):\n    \"\"\"\n    First, it builds one decision tree per feature, to predict the target\n    Second, it makes predictions using the decision tree and the mentioned feature\n    Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n    It selects the highest ranked features\n    \"\"\"\n    df = df.select_dtypes(include='number') # NB: Only includes numeric\n    X_train, X_test, y_train, y_test= train_test_split(df.drop(columns = ['total_points']), df['total_points'], test_size=0.2) \n    mse_values = []",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "remove_multicoll",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def remove_multicoll(df):\n    \"\"\"[This function removes columns with ]\n    Args:\n        df ([type]): [description]\n    \"\"\"    \n    df = df.select_dtypes(include='number')\n    plt.figure(figsize=(20,20))\n    corrplot(df.corr(), size_scale=700, marker='s')\ndef PLS_regression(data):\n    \"\"\"[For removing multicollinearityt]",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "PLS_regression",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def PLS_regression(data):\n    \"\"\"[For removing multicollinearityt]\n    Args:\n        data ([type]): [description]\n    \"\"\"    \n    #define predictor and response variables\n    X = data.select_dtypes(include='number').drop('total_points', axis = 1)\n    y = data['total_points']\n    #define cross-validation method\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "calculate_vif",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def calculate_vif(data):\n    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])\n    x_var_names = data.columns\n    for i in range(0, x_var_names.shape[0]):\n        y = data[x_var_names[i]]\n        x = data[x_var_names.drop([x_var_names[i]])]\n        r_squared = sm.OLS(y,x).fit().rsquared\n        vif = round(1/(1-r_squared),2)\n        vif_df.loc[i] = [x_var_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "boruta_fs",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def boruta_fs(df, df_test):\n    # The important point is for BorutaPy, multicollinearity should be removed before running it.\n    X_train, y_train, X_test, y_test = split_data_by_GW(df, df_test) # Where X_train and y_train contain the entire season\n    forest = RandomForestRegressor(n_jobs=-1,  max_depth=5)\n    forest.fit(X_train, y_train)\n    feat_selector = BorutaPy(forest, n_estimators='auto', verbose=2, random_state=1)\n    feat_selector.fit(np.array(X_train), np.array(y_train))   # find all relevant features\n    feat_selector.support_ # check selected features\n    feat_selector.ranking_ # check ranking of features\n    X_filtered = feat_selector.transform(X_train)    # call transform() on X to filter it down to selected features",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "clean_dataset",
        "kind": 2,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "def clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)\nfpl_19, understat_19 = read_data('2019-20')\nfpl_20, understat_20 = read_data('2020-21')\n# %%\nfpl_19 = one_hot_encode(fpl_19.drop('kickoff_time', axis = 1)).reset_index(drop=True)\nfpl_20 = one_hot_encode(fpl_20.drop('kickoff_time', axis = 1)).reset_index(drop = True)",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "fpl_19",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "fpl_19 = one_hot_encode(fpl_19.drop('kickoff_time', axis = 1)).reset_index(drop=True)\nfpl_20 = one_hot_encode(fpl_20.drop('kickoff_time', axis = 1)).reset_index(drop = True)\nboruta_fs(clean_dataset(fpl_19), clean_dataset(fpl_20))\n# %%\n# %%\n%%time\nX = fpl.select_dtypes(include='number').drop('total_points', axis = 1).to_numpy()\ny = fpl['total_points'].to_numpy()\ngmm = inf.get_gmm(X, y)\nselect = inf.SelectVars(gmm, selection_mode = 'backward')",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "fpl_20",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "fpl_20 = one_hot_encode(fpl_20.drop('kickoff_time', axis = 1)).reset_index(drop = True)\nboruta_fs(clean_dataset(fpl_19), clean_dataset(fpl_20))\n# %%\n# %%\n%%time\nX = fpl.select_dtypes(include='number').drop('total_points', axis = 1).to_numpy()\ny = fpl['total_points'].to_numpy()\ngmm = inf.get_gmm(X, y)\nselect = inf.SelectVars(gmm, selection_mode = 'backward')\n# %%",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "X = fpl.select_dtypes(include='number').drop('total_points', axis = 1).to_numpy()\ny = fpl['total_points'].to_numpy()\ngmm = inf.get_gmm(X, y)\nselect = inf.SelectVars(gmm, selection_mode = 'backward')\n# %%\nselect.fit(X, y, verbose=True)    \nselect.get_info()\nselect.plot_mi()\nselect.plot_delta()\n# %%",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "y = fpl['total_points'].to_numpy()\ngmm = inf.get_gmm(X, y)\nselect = inf.SelectVars(gmm, selection_mode = 'backward')\n# %%\nselect.fit(X, y, verbose=True)    \nselect.get_info()\nselect.plot_mi()\nselect.plot_delta()\n# %%\nX_new = select.transform(X, rd=2)",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "gmm",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "gmm = inf.get_gmm(X, y)\nselect = inf.SelectVars(gmm, selection_mode = 'backward')\n# %%\nselect.fit(X, y, verbose=True)    \nselect.get_info()\nselect.plot_mi()\nselect.plot_delta()\n# %%\nX_new = select.transform(X, rd=2)\nX_new.shape",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "select",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "select = inf.SelectVars(gmm, selection_mode = 'backward')\n# %%\nselect.fit(X, y, verbose=True)    \nselect.get_info()\nselect.plot_mi()\nselect.plot_delta()\n# %%\nX_new = select.transform(X, rd=2)\nX_new.shape\n# %%",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "X_new",
        "kind": 5,
        "importPath": "src.select_data",
        "description": "src.select_data",
        "peekOfCode": "X_new = select.transform(X, rd=2)\nX_new.shape\n# %%",
        "detail": "src.select_data",
        "documentation": {}
    },
    {
        "label": "season",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "season = '2019-20'\ntraining_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\nfpl = pd.read_csv(training_path + 'shifted_fpl.csv', index_col=0)\n# %%\n# load data\ndata  = fpl.select_dtypes(include='number')\nfeat  = np.asarray(data.drop('total_points_shift', axis=1))\nlabel = np.asarray(data['total_points_shift'])\n# %%\n# split data into train & validation (70 -- 30)",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "training_path",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "training_path = f'C://Users//jd-vz//Desktop//Code//data//{season}//training//'\nfpl = pd.read_csv(training_path + 'shifted_fpl.csv', index_col=0)\n# %%\n# load data\ndata  = fpl.select_dtypes(include='number')\nfeat  = np.asarray(data.drop('total_points_shift', axis=1))\nlabel = np.asarray(data['total_points_shift'])\n# %%\n# split data into train & validation (70 -- 30)\nxtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.3)",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "fpl",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "fpl = pd.read_csv(training_path + 'shifted_fpl.csv', index_col=0)\n# %%\n# load data\ndata  = fpl.select_dtypes(include='number')\nfeat  = np.asarray(data.drop('total_points_shift', axis=1))\nlabel = np.asarray(data['total_points_shift'])\n# %%\n# split data into train & validation (70 -- 30)\nxtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.3)\nfold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "label",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "label = np.asarray(data['total_points_shift'])\n# %%\n# split data into train & validation (70 -- 30)\nxtrain, xtest, ytrain, ytest = train_test_split(feat, label, test_size=0.3)\nfold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\n# # parameter\nk    = 5     # k-value in KNN\nN    = 10    # number of chromosomes\nT    = 50   # maximum number of generations\n# CR   = 0.8",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "fold",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "fold = {'xt':xtrain, 'yt':ytrain, 'xv':xtest, 'yv':ytest}\n# # parameter\nk    = 5     # k-value in KNN\nN    = 10    # number of chromosomes\nT    = 50   # maximum number of generations\n# CR   = 0.8\n# MR   = 0.01\n# opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'CR':CR, 'MR':MR}\nc1  = 2         # cognitive factor\nc2  = 2         # social factor ",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "opts",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "opts = {'k':k, 'fold':fold, 'N':N, 'T':T, 'w':w, 'c1':c1, 'c2':c2}\n# perform feature selection\nfmdl = jfs(feat, label, opts)\nsf   = fmdl['sf']\n# %%\nprint('Selected Features:', data.columns[fmdl['sf']].values)\nprint('Total Features:', len(data.columns[fmdl['sf']].values))\n# %%\n# model with selected features\nnum_train = np.size(xtrain, 0)",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "fmdl",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "fmdl = jfs(feat, label, opts)\nsf   = fmdl['sf']\n# %%\nprint('Selected Features:', data.columns[fmdl['sf']].values)\nprint('Total Features:', len(data.columns[fmdl['sf']].values))\n# %%\n# model with selected features\nnum_train = np.size(xtrain, 0)\nnum_valid = np.size(xtest, 0)\nx_train   = xtrain[:, sf]",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "num_train",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "num_train = np.size(xtrain, 0)\nnum_valid = np.size(xtest, 0)\nx_train   = xtrain[:, sf]\ny_train   = ytrain.reshape(num_train)  # Solve bug\nx_valid   = xtest[:, sf]\ny_valid   = ytest.reshape(num_valid)  # Solve bug\nmdl       = KNeighborsRegressor(n_neighbors = k) \nmdl.fit(x_train, y_train)\n# %%\n# accuracy",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "num_valid",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "num_valid = np.size(xtest, 0)\nx_train   = xtrain[:, sf]\ny_train   = ytrain.reshape(num_train)  # Solve bug\nx_valid   = xtest[:, sf]\ny_valid   = ytest.reshape(num_valid)  # Solve bug\nmdl       = KNeighborsRegressor(n_neighbors = k) \nmdl.fit(x_train, y_train)\n# %%\n# accuracy\nimport sklearn.metrics as metrics",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "num_feat",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "num_feat = fmdl['nf']\nprint(\"Feature Size:\", num_feat)\n# %%\n# plot convergence\ncurve   = fmdl['c']\ncurve   = curve.reshape(np.size(curve,1))\nx       = np.arange(0, opts['T'], 1.0) + 1.0\nfig, ax = plt.subplots()\nax.plot(x, curve, 'o-')\nax.set_xlabel('Number of Iterations')",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "r = relief.()\nr.fit_transform(X_train, y_train)\n# %%\nimport sklearn_relief as relief\nr = relief.RReliefF(\n    n_features=3 # Choose the best 3 features\n) # Will run by default on all processors concurrently\nmy_transformed_matrix = r.fit_transform(\n    X_train,\n    y_train",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "r",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "r = relief.RReliefF(\n    n_features=3 # Choose the best 3 features\n) # Will run by default on all processors concurrently\nmy_transformed_matrix = r.fit_transform(\n    X_train,\n    y_train\n)\nprint(my_transformed_matrix)\n# %%",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "my_transformed_matrix",
        "kind": 5,
        "importPath": "src.select_data_wrappers",
        "description": "src.select_data_wrappers",
        "peekOfCode": "my_transformed_matrix = r.fit_transform(\n    X_train,\n    y_train\n)\nprint(my_transformed_matrix)\n# %%",
        "detail": "src.select_data_wrappers",
        "documentation": {}
    },
    {
        "label": "forwardSelection",
        "kind": 2,
        "importPath": "src.stepwiseSelection",
        "description": "src.stepwiseSelection",
        "peekOfCode": "def forwardSelection(X, y, model_type =\"linear\",elimination_criteria = \"aic\", varchar_process = \"dummy_dropfirst\", sl=0.05):\n    \"\"\"\n    Forward Selection is a function, based on regression models, that returns significant features and selection iterations.\\n\n    Required Libraries: pandas, numpy, statmodels\n    Parameters\n    ----------\n    X : Independent variables (Pandas Dataframe)\\n\n    y : Dependent variable (Pandas Series, Pandas Dataframe)\\n\n    model_type : 'linear' or 'logistic'\\n\n    elimination_criteria : 'aic', 'bic', 'r2', 'adjr2' or None\\n",
        "detail": "src.stepwiseSelection",
        "documentation": {}
    },
    {
        "label": "backwardSelection",
        "kind": 2,
        "importPath": "src.stepwiseSelection",
        "description": "src.stepwiseSelection",
        "peekOfCode": "def backwardSelection(X, y, model_type =\"linear\",elimination_criteria = \"aic\", varchar_process = \"dummy_dropfirst\", sl=0.05):\n    \"\"\"\n    Backward Selection is a function, based on regression models, that returns significant features and selection iterations.\\n\n    Required Libraries: pandas, numpy, statmodels\n    Parameters\n    ----------\n    X : Independent variables (Pandas Dataframe)\\n\n    y : Dependent variable (Pandas Series, Pandas Dataframe)\\n\n    model_type : 'linear' or 'logistic'\\n\n    elimination_criteria : 'aic', 'bic', 'r2', 'adjr2' or None\\n",
        "detail": "src.stepwiseSelection",
        "documentation": {}
    },
    {
        "label": "n_samples",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "n_samples = 200\noutliers_fraction = 0.25\nclusters_separation = [0]\n# Compare given detectors under given settings\n# Initialize the data\nxx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "outliers_fraction",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "outliers_fraction = 0.25\nclusters_separation = [0]\n# Compare given detectors under given settings\n# Initialize the data\nxx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "clusters_separation",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "clusters_separation = [0]\n# Compare given detectors under given settings\n# Initialize the data\nxx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP\ndetector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "n_inliers",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "n_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP\ndetector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n                 LOF(n_neighbors=50)]\n# Show the statics of the data",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "n_outliers",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "n_outliers = int(outliers_fraction * n_samples)\nground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP\ndetector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n                 LOF(n_neighbors=50)]\n# Show the statics of the data\nprint('Number of inliers: %i' % n_inliers)",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "ground_truth",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "ground_truth = np.zeros(n_samples, dtype=int)\nground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP\ndetector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n                 LOF(n_neighbors=50)]\n# Show the statics of the data\nprint('Number of inliers: %i' % n_inliers)\nprint('Number of outliers: %i' % n_outliers)",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "ground_truth[-n_outliers:]",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "ground_truth[-n_outliers:] = 1\n# initialize a set of detectors for LSCP\ndetector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n                 LOF(n_neighbors=50)]\n# Show the statics of the data\nprint('Number of inliers: %i' % n_inliers)\nprint('Number of outliers: %i' % n_outliers)\nprint(",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "detector_list",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),\n                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),\n                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),\n                 LOF(n_neighbors=50)]\n# Show the statics of the data\nprint('Number of inliers: %i' % n_inliers)\nprint('Number of outliers: %i' % n_outliers)\nprint(\n    'Ground truth shape is {shape}. Outlier are 1 and inliers are 0.\\n'.format(\n        shape=ground_truth.shape))",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "random_state",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "random_state = 42\n# Define nine outlier detection tools to be compared\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)':\n        ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)':\n        CBLOF(contamination=outliers_fraction,\n              check_estimator=False, random_state=random_state),\n    'Feature Bagging':\n        FeatureBagging(LOF(n_neighbors=35),",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "classifiers",
        "kind": 5,
        "importPath": "src.test",
        "description": "src.test",
        "peekOfCode": "classifiers = {\n    'Angle-based Outlier Detector (ABOD)':\n        ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)':\n        CBLOF(contamination=outliers_fraction,\n              check_estimator=False, random_state=random_state),\n    'Feature Bagging':\n        FeatureBagging(LOF(n_neighbors=35),\n                       contamination=outliers_fraction,\n                       random_state=random_state),",
        "detail": "src.test",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "df = pd.read_csv('C://Users//jd-vz//Desktop//Code//data//2019-20//training//cleaned_imp.csv', index_col=0)\nscaler = StandardScaler()\ndf[df.drop(columns=['GW']).select_dtypes(\"number\").columns] = scaler.fit_transform(df.drop(columns=['GW']).select_dtypes(\"number\"))\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# OHE categorical\ndf = pd.get_dummies(df, columns=['was_home', 'position', 'team_h', 'team_a'], \n                    prefix=['home', 'position', 'team_h', 'team_a'])\ndf.drop(columns=['home_False'], axis=1, inplace=True)\n# %%",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "scaler = StandardScaler()\ndf[df.drop(columns=['GW']).select_dtypes(\"number\").columns] = scaler.fit_transform(df.drop(columns=['GW']).select_dtypes(\"number\"))\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# OHE categorical\ndf = pd.get_dummies(df, columns=['was_home', 'position', 'team_h', 'team_a'], \n                    prefix=['home', 'position', 'team_h', 'team_a'])\ndf.drop(columns=['home_False'], axis=1, inplace=True)\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "df[df.drop(columns=['GW']).select_dtypes(\"number\").columns]",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "df[df.drop(columns=['GW']).select_dtypes(\"number\").columns] = scaler.fit_transform(df.drop(columns=['GW']).select_dtypes(\"number\"))\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# OHE categorical\ndf = pd.get_dummies(df, columns=['was_home', 'position', 'team_h', 'team_a'], \n                    prefix=['home', 'position', 'team_h', 'team_a'])\ndf.drop(columns=['home_False'], axis=1, inplace=True)\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# Split",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "df = pd.get_dummies(df, columns=['was_home', 'position', 'team_h', 'team_a'], \n                    prefix=['home', 'position', 'team_h', 'team_a'])\ndf.drop(columns=['home_False'], axis=1, inplace=True)\n# %%\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n# Split\nX = df.drop(columns=['GW', 'player_name', 'total_points', 'kickoff_time'])\ny = df[\"total_points\"]\n# %%\n%%time",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "X = df.drop(columns=['GW', 'player_name', 'total_points', 'kickoff_time'])\ny = df[\"total_points\"]\n# %%\n%%time\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>#\n# Try RFE\nrandom_state = 0 \n# estimator = LinearRegression()\nestimator = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was fitted to scaled data\nrfecv = RFECV(estimator=estimator, cv=3,verbose = 2)",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "y = df[\"total_points\"]\n# %%\n%%time\n# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>#\n# Try RFE\nrandom_state = 0 \n# estimator = LinearRegression()\nestimator = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was fitted to scaled data\nrfecv = RFECV(estimator=estimator, cv=3,verbose = 2)\nrfecv.fit(X, y)",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "random_state",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "random_state = 0 \n# estimator = LinearRegression()\nestimator = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was fitted to scaled data\nrfecv = RFECV(estimator=estimator, cv=3,verbose = 2)\nrfecv.fit(X, y)\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.grid()\nplt.xticks(range(1, X.shape[1]+1))\nplt.xlabel(\"Number of Selected Features\")",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "estimator",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "estimator = SVR(C=100, epsilon=0.01, gamma=0.005, verbose=True, kernel='rbf') # Note: This was fitted to scaled data\nrfecv = RFECV(estimator=estimator, cv=3,verbose = 2)\nrfecv.fit(X, y)\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.grid()\nplt.xticks(range(1, X.shape[1]+1))\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"CV Score\")\nplt.title(\"Recursive Feature Elimination (RFE)\")",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "rfecv",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "rfecv = RFECV(estimator=estimator, cv=3,verbose = 2)\nrfecv.fit(X, y)\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.grid()\nplt.xticks(range(1, X.shape[1]+1))\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"CV Score\")\nplt.title(\"Recursive Feature Elimination (RFE)\")\nplt.show()",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "X_rfe",
        "kind": 5,
        "importPath": "src.test_fs",
        "description": "src.test_fs",
        "peekOfCode": "X_rfe = X.iloc[:, rfecv.support_] # Selected features\n# %%\nprint(\"\\\"X\\\" dimension: {}\".format(X.shape))\nprint(\"\\\"X\\\" column list:\", X.columns.tolist())\nprint(\"\\\"X_rfe\\\" dimension: {}\".format(X_rfe.shape))\nprint(\"\\\"X_rfe\\\" column list:\", X_rfe.columns.tolist())\n# %%\nimport thundersvm\n# %%\nfrom thundersvm import SVR",
        "detail": "src.test_fs",
        "documentation": {}
    },
    {
        "label": "set_season_time",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def set_season_time(season):\n    \"\"\"[This function specifies the start and ending dates of the season]\n    Args:\n        season ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    if season == '2020-21':\n        startdate = time.strptime('12-08-2020', '%d-%m-%Y')\n        startdate = datetime.fromtimestamp(mktime(startdate))",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "dlt_create_dir",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def dlt_create_dir(path):\n    \"\"\"[This function deletes (if existing) and creates a directory]\n    Args:\n        path ([type]): [description]\n    \"\"\"    \n    shutil.rmtree(path,ignore_errors=True)\n    os.makedirs(path, exist_ok = True)\ndef delete_any_duplicates(df):\n    \"\"\"[Hardcoded solution to a problem within the code]\n    Args:",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "delete_any_duplicates",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def delete_any_duplicates(df):\n    \"\"\"[Hardcoded solution to a problem within the code]\n    Args:\n        df ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    df1 = df[df.columns[~df.columns.str.endswith(tuple([str(i) for i in range(10)]))]]\n    return df1\ndef one_hot_encode(fpl):",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "one_hot_encode",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def one_hot_encode(fpl):\n    \"\"\"[This function one hot encodes the four categorical features into dummy variables]\n    Args:\n        fpl ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    fpl = pd.get_dummies(fpl, columns=['was_home', 'position', 'team_h', 'team_a'], \n                      prefix=['home', 'position', 'team_h', 'team_a'])\n    fpl.drop(columns=['home_False'], axis=1, inplace=True)",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "split_data_by_GW",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def split_data_by_GW(df, df_test, GW = 1):\n    \"\"\"[This function splits the data according to ]\n    Args:\n        df ([type]): [description]\n        df_test ([type]): [description]\n        GW (int, optional): [description]. Defaults to 1.\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    df = df.append(df_test[df_test['GW'] < GW])",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "impute_by_player",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def impute_by_player(df):\n    # Note: Discontinued\n    \"\"\"[Imputes missing features based on player performance]\n    Args:\n        understat_miss_merged ([type]): [Dataframe with player-specific missing values]\n    Returns:\n        [type]: []\n    \"\"\"    \n    empty_feats = df.columns[df.isnull().any()].tolist() \n    for feat in empty_feats:",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "change_team_strength",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def change_team_strength(teams, fpl):\n    \"\"\"[This function creates the strength statistics featuress]\n    Args:\n        teams ([type]): [description]\n        fpl ([type]): [description]\n    Returns:\n        [type]: [description]\n    \"\"\"    \n    team_h_strength_attack = []\n    team_h_strength_defense = []",
        "detail": "src.utilities",
        "documentation": {}
    },
    {
        "label": "union",
        "kind": 2,
        "importPath": "src.utilities",
        "description": "src.utilities",
        "peekOfCode": "def union(a, b):\n    \"\"\"[This function finds the union between two player name columns]\n    Args:\n        a ([type]): [description]\n        b ([type]): [description]\n    Returns:\n        [type]: [The union]\n    \"\"\"    \n    print(len(list(set(a) | set(b))), 'unique, not necessarily matching names between FPL and Understat')\n    return list(set(a) | set(b))",
        "detail": "src.utilities",
        "documentation": {}
    }
]