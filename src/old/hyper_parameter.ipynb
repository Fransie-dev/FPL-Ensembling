{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "hyper_parameter.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9sQxCWH1csG"
      },
      "source": [
        "Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irym_CdSqzYU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mnlt2BJyEtv"
      },
      "source": [
        "Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAj4HQ-RqvUc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import sklearn.metrics as metrics\n",
        "from keras.backend import clear_session\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.svm import SVR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "import joblib\n",
        "\n",
        "def scale_datasets(X_train, y_train):\n",
        "  \"\"\"\n",
        "  Standard Scale test and train data\n",
        "  Z - Score normalization\n",
        "  \"\"\"\n",
        "  std_pred, std_resp = StandardScaler(), StandardScaler()\n",
        "  X_train, y_train = std_pred.fit_transform(X_train), std_resp.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
        "  return X_train, y_train\n",
        "\n",
        "  \n",
        "df = pd.read_csv('./drive//MyDrive//Data//collected_us.csv')\n",
        "df = df[df['kickoff_time'] <= '2020-07-26'] # Note: Look at me\n",
        "df.drop(['player_name', 'GW', 'kickoff_time'], axis=1, inplace=True)\n",
        "std_pred, std_resp = StandardScaler(), StandardScaler()\n",
        "X_train, y_train = df.select_dtypes(include = 'number').drop('total_points', axis=1), df['total_points'] # NOTE: look at me\n",
        "X_train, y_train = X_train.iloc[0:200, 0:10], y_train.iloc[0:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZye4U0xrSJ"
      },
      "source": [
        "Random Forest Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My-V6YaVqvUh"
      },
      "source": [
        "rf_n_estimators = [int(x) for x in np.linspace(200, 1000, 5)]\n",
        "rf_n_estimators.append(1500)\n",
        "rf_n_estimators.append(2000)\n",
        "rf_max_depth = [int(x) for x in np.linspace(5, 55, 11)]\n",
        "rf_max_depth.append(None)\n",
        "rf_max_features = ['auto', 'sqrt', 'log2']\n",
        "rf_criterion = ['mse', 'mae']\n",
        "rf_min_samples_split = [int(x) for x in np.linspace(2, 10, 9)]\n",
        "rf_min_impurity_decrease = [0.0, 0.05, 0.1]\n",
        "rf_bootstrap = [True, False]\n",
        "rf_grid = {'n_estimators': rf_n_estimators,\n",
        "               'max_depth': rf_max_depth,\n",
        "               'max_features': rf_max_features,\n",
        "               'criterion': rf_criterion,\n",
        "               'min_samples_split': rf_min_samples_split,\n",
        "               'min_impurity_decrease': rf_min_impurity_decrease,\n",
        "               'bootstrap': rf_bootstrap}\n",
        "rf_base = RandomForestRegressor()\n",
        "rf_random = RandomizedSearchCV(estimator = rf_base, param_distributions = rf_grid, \n",
        "                               n_iter = 10, cv = 3, verbose = 2, random_state = 42, \n",
        "                               n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)\n",
        "joblib.dump(rf_random, 'random_forst_search.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LTcGWFqvUi"
      },
      "source": [
        "rf_results = joblib.load('./random_forst_search.pkl')\n",
        "rf_results.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ787IKGx8AM"
      },
      "source": [
        "Support Vector Regression Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TkMMxVW-ZRp"
      },
      "source": [
        "! pip install thundersvm\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "!ls  # Check if required cuda 9.0 amd64-deb file is downloaded\n",
        "!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "!ls /var/cuda-repo-9-0-local | grep .pub\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!sudo apt-get install cuda-9.0\n",
        "from thundersvm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN6KpC4m__r4"
      },
      "source": [
        "from thundersvm import SVR\n",
        "import multiprocessing\n",
        "if __name__ == '__main__':\n",
        "  df = pd.read_csv('./drive//MyDrive//Data//collected_us.csv')\n",
        "  df = df[df['kickoff_time'] <= '2020-07-26'] # Note: Look at me\n",
        "  df.drop(['player_name', 'GW', 'kickoff_time'], axis=1, inplace=True)\n",
        "  std_pred, std_resp = StandardScaler(), StandardScaler()\n",
        "  X_train, y_train = df.select_dtypes(include = 'number').drop('total_points', axis=1), df['total_points'] # NOTE: look at me\n",
        "  X_train, y_train = std_pred.fit_transform(X_train.iloc[0:200, 0:10]), std_resp.fit_transform(y_train.iloc[0:200].to_numpy().reshape(-1, 1))\n",
        "\n",
        "\n",
        "  hyperparameters = {'gamma': [float(x) for x in np.linspace(0.0, 0.3, 5)], \n",
        "                      'C': [float(x) for x in np.linspace(0.0, 1000, 5)],\n",
        "                    'epsilon': [float(x) for x in np.linspace(0.0, 1, 5)],\n",
        "                    'kernel': ['rbf', 'poly', 'linear']}\n",
        "  grid = RandomizedSearchCV(estimator = SVR(verbose=2),param_distributions = hyperparameters,\n",
        "                            n_iter = 10, cv = 3,  verbose = 2, random_state = 42, n_jobs=8)\n",
        "  grid.fit(X_train,y_train)\n",
        "  print(grid.best_estimator_)\n",
        "  print(grid.best_score_)\n",
        "  joblib.dump(grid, 'svr.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdHiY4A-mlBr"
      },
      "source": [
        "Keras Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F2-l1PpmkaM"
      },
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9lW5YoimsYN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaqxfBaJw7-L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVCpjEBkvm9-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NvEi8F1wXac"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z2sbH4vSBn6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt\n",
        "\n",
        "def tune_nn_model(hp, MAX_LAYERS = 2):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(hp.Int('num_layers', 1, MAX_LAYERS)):\n",
        "\n",
        "      units = hp.Int('units_' + str(i),\n",
        "                     min_value=32,\n",
        "                     max_value=512,\n",
        "                     step=120,\n",
        "                     default = 512)\n",
        "      \n",
        "      model.add(keras.layers.Dense(units=units, activation='relu',\n",
        "                                   kernel_initializer=tf.keras.initializers.HeUniform()))\n",
        "\n",
        "      drop_rate = hp.Choice('drop_rate_' + str(i), [0.0, 0.1, 0.3],default = 0)\n",
        "      model.add(keras.layers.Dropout(rate=drop_rate))\n",
        "\n",
        "      l1_reg = hp.Choice('l1_reg_' + str(i), [0.0, 0.01, 0.001],default = 0)\n",
        "      model.add(keras.layers.ActivityRegularization(l1 = l1_reg))\n",
        "      \n",
        "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'SGD', 'rmsprop'])\n",
        "    optimizer = tf.keras.optimizers.get(hp_optimizer)\n",
        "    optimizer.learning_rate = hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4], default=0.01)\n",
        "\n",
        "    model.add(keras.layers.Dense(1, activation='linear',kernel_initializer=tf.keras.initializers.HeUniform()))\n",
        "    model.compile(optimizer=optimizer, loss = 'mse', metrics = ['mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "MAX_TRIALS = 1\n",
        "MAX_EXECUTIONS = 1\n",
        "EPOCHS = 1\n",
        "MAX_LAYERS = 2\n",
        "\n",
        "b_tuner = kt.BayesianOptimization(\n",
        "    tune_nn_model,\n",
        "    objective='mse',\n",
        "    max_trials=MAX_TRIALS,\n",
        "    executions_per_trial=MAX_EXECUTIONS,\n",
        "    directory='test_dir',\n",
        "    project_name='b_tune_nn',\n",
        "    seed=0,\n",
        "    overwrite = True,\n",
        "    num_initial_points=2, alpha=0.0001, beta=2.6\n",
        ")\n",
        "\n",
        "X_train, y_train = df.select_dtypes(include = 'number').drop('total_points', axis=1), df['total_points'] # NOTE: look at me\n",
        "X_train, y_train = scale_datasets(X_train, y_train)\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "b_tuner.search(X_train[0:100], y_train[0:100], epochs=EPOCHS, validation_split=0.2, callbacks=[stop_early])\n",
        "joblib.dump(b_tuner, 'neural_net_bayes.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdO0zAYdbX6i"
      },
      "source": [
        "tuner = joblib.load('./neural_net_bayes.pkl')\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps) \n",
        "print(model)\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2) # Note this should be long enough\n",
        "val_acc_per_epoch = history.history['val_loss']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))\n",
        "\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [Total points]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()\n",
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Aie3SIn6vr"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "X_train, y_train = df.select_dtypes(include = 'number').drop('total_points', axis=1), df['total_points'] \n",
        "X_train, y_train = X_train.iloc[0:100,0:5], y_train.iloc[0:100]\n",
        "X_train, y_train = scale_datasets(X_train, y_train)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'base_estimator__C' : [0.1, 1, 10, 100],\n",
        "    'base_estimator__gamma' : [0.1, 1, 10, 100],\n",
        "    'base_estimator__epsilon' : [0.1, 1, 10, 100],\n",
        "    'n_estimators' : [10, 50, 100],\n",
        "    'bootstrap' : [True, False]\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "bagged_svr = GridSearchCV(BaggingRegressor(SVR()), param_grid, n_jobs= 4, cv = 3,verbose = 3).fit(X_train, y_train)\n",
        "joblib.dump(bagged_svr, 'bagged_svr.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhqWt-8StHth"
      },
      "source": [
        "bagged_svr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVtqgbkMtW_y"
      },
      "source": [
        "kNN Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zSF146PtbMn"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "X_train, y_train = df.select_dtypes(include = 'number').drop('total_points', axis=1), df['total_points'] \n",
        "# X_train, y_train = X_train.iloc[0:100,0:5], y_train.iloc[0:100]\n",
        "X_train, y_train = scale_datasets(X_train, y_train)\n",
        "\n",
        "estimator_KNN = KNeighborsRegressor(algorithm='kd_tree')\n",
        "\n",
        "parameters_KNN = {\n",
        "    'n_neighbors': (1,12, 1),\n",
        "    'leaf_size': (20,40,1),\n",
        "    'p': (1,2),\n",
        "    'weights': ('uniform', 'distance'),\n",
        "    'metric': ('minkowski', 'chebyshev')}\n",
        "                   \n",
        "# with GridSearch\n",
        "grid_search_KNN = GridSearchCV(estimator=estimator_KNN,\n",
        "    param_grid=parameters_KNN, n_jobs= 4, cv = 5, verbose = 3).fit(X_train, y_train)\n",
        "grid_search_KNN.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZCiTTuOz1Di"
      },
      "source": [
        "grid_search_KNN.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37dWh1SwBpl"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CVWuH2_wDDa"
      },
      "source": [
        "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
        "grid_lr = GridSearchCV(LinearRegression(), parameters, cv=5).fit(X_train, y_train)\n",
        "grid_lr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFv-YvbWz0Cf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}